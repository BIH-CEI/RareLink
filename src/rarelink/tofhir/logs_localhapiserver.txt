2025-01-13 22:40:16 2025-01-13_19:40:16.042 [main] INFO  i.t.e.Boot$ - Starting toFHIR version: 1.1-SNAPSHOT
2025-01-13 22:40:20 2025-01-13_19:40:20.466 [toFHIR-akka.actor.default-dispatcher-16] INFO  i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (STARTED) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'ips.patient'!
2025-01-13 22:40:20 
2025-01-13 22:40:20 2025-01-13_19:40:20.469 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Reading source data for mapping ips.patient within mapping job rarelink-cdm ...
2025-01-13 22:40:39 2025-01-13_19:40:39.949 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - 5 records read for mapping ips.patient within mapping job rarelink-cdm ...
2025-01-13 22:40:39 2025-01-13_19:40:39.961 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Executing the mapping ips.patient within job rarelink-cdm ...
2025-01-13 22:40:40 2025-01-13_19:40:40.803 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.d.w.FhirRepositoryWriter - Created FHIR resources will be written to the given FHIR repository URL:http://172.21.0.2:8080/fhir
2025-01-13 22:40:48 2025-01-13_19:40:48.212 [Executor task launch worker for task 11.0 in stage 14.0 (TID 615)] DEBUG i.t.e.d.w.FhirRepositoryWriter - Batch Update request will be sent to the FHIR repository for 1 resources.
2025-01-13 22:40:48 2025-01-13_19:40:48.541 [Executor task launch worker for task 11.0 in stage 14.0 (TID 615)] DEBUG i.t.e.d.w.FhirRepositoryWriter - 1 FHIR resources were written to the FHIR repository successfully.
2025-01-13 22:40:49 2025-01-13_19:40:49.587 [Executor task launch worker for task 82.0 in stage 14.0 (TID 686)] DEBUG i.t.e.d.w.FhirRepositoryWriter - Batch Update request will be sent to the FHIR repository for 1 resources.
2025-01-13 22:40:49 2025-01-13_19:40:49.636 [Executor task launch worker for task 88.0 in stage 14.0 (TID 692)] DEBUG i.t.e.d.w.FhirRepositoryWriter - Batch Update request will be sent to the FHIR repository for 1 resources.
2025-01-13 22:40:49 2025-01-13_19:40:49.733 [Executor task launch worker for task 82.0 in stage 14.0 (TID 686)] DEBUG i.t.e.d.w.FhirRepositoryWriter - 1 FHIR resources were written to the FHIR repository successfully.
2025-01-13 22:40:49 2025-01-13_19:40:49.771 [Executor task launch worker for task 88.0 in stage 14.0 (TID 692)] DEBUG i.t.e.d.w.FhirRepositoryWriter - 1 FHIR resources were written to the FHIR repository successfully.
2025-01-13 22:40:49 2025-01-13_19:40:49.794 [Executor task launch worker for task 99.0 in stage 14.0 (TID 703)] DEBUG i.t.e.d.w.FhirRepositoryWriter - Batch Update request will be sent to the FHIR repository for 1 resources.
2025-01-13 22:40:49 2025-01-13_19:40:49.847 [Executor task launch worker for task 99.0 in stage 14.0 (TID 703)] DEBUG i.t.e.d.w.FhirRepositoryWriter - 1 FHIR resources were written to the FHIR repository successfully.
2025-01-13 22:40:50 2025-01-13_19:40:50.159 [Executor task launch worker for task 145.0 in stage 14.0 (TID 749)] DEBUG i.t.e.d.w.FhirRepositoryWriter - Batch Update request will be sent to the FHIR repository for 1 resources.
2025-01-13 22:40:50 2025-01-13_19:40:50.341 [Executor task launch worker for task 145.0 in stage 14.0 (TID 749)] DEBUG i.t.e.d.w.FhirRepositoryWriter - 1 FHIR resources were written to the FHIR repository successfully.
2025-01-13 22:40:59 2025-01-13_19:40:59.534 [toFHIR-akka.actor.default-dispatcher-16] INFO  i.t.e.e.l.ExecutionLogger$ - toFHIR chunk mapping result (SUCCESS) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'ips.patient'!
2025-01-13 22:40:59     # of Invalid Rows:      0
2025-01-13 22:40:59     # of Not Mapped:        0
2025-01-13 22:40:59     # of Failed writes:     0
2025-01-13 22:40:59     # of Written FHIR resources:    5
2025-01-13 22:40:59 2025-01-13_19:40:59.593 [toFHIR-akka.actor.default-dispatcher-16] INFO  i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (SUCCESS) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'ips.patient'!
2025-01-13 22:40:59     # of Invalid Rows:      0
2025-01-13 22:40:59     # of Not Mapped:        0
2025-01-13 22:40:59     # of Failed writes:     0
2025-01-13 22:40:59     # of Written FHIR resources:    5
2025-01-13 22:40:59 2025-01-13_19:40:59.596 [toFHIR-akka.actor.default-dispatcher-16] INFO  i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (STARTED) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'observation.age_category'!
2025-01-13 22:40:59 
2025-01-13 22:40:59 2025-01-13_19:40:59.597 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Reading source data for mapping observation.age_category within mapping job rarelink-cdm ...
2025-01-13 22:40:17 WARNING: An illegal reflective access operation has occurred
2025-01-13 22:40:17 WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/tofhir/tofhir-engine-standalone.jar) to constructor java.nio.DirectByteBuffer(long,int)
2025-01-13 22:40:17 WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
2025-01-13 22:40:17 WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
2025-01-13 22:40:17 WARNING: All illegal access operations will be denied in a future release
2025-01-13 22:41:01 2025-01-13_19:41:01.195 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - 5 records read for mapping observation.age_category within mapping job rarelink-cdm ...
2025-01-13 22:41:01 2025-01-13_19:41:01.196 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Executing the mapping observation.age_category within job rarelink-cdm ...
2025-01-13 22:41:01 2025-01-13_19:41:01.351 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.d.w.FhirRepositoryWriter - Created FHIR resources will be written to the given FHIR repository URL:http://172.21.0.2:8080/fhir
2025-01-13 22:41:01 2025-01-13_19:41:01.774 [Executor task launch worker for task 0.0 in stage 35.0 (TID 1411)] DEBUG i.t.e.d.w.FhirRepositoryWriter - Batch Update request will be sent to the FHIR repository for 5 resources.
2025-01-13 22:41:01 2025-01-13_19:41:01.889 [Executor task launch worker for task 0.0 in stage 35.0 (TID 1411)] ERROR o.a.s.e.Executor - Exception in task 0.0 in stage 35.0 (TID 1411)
2025-01-13 22:41:01 java.util.NoSuchElementException: None.get
2025-01-13 22:41:01     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:01     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:01     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:01     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:01     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:01     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:01     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:01     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:01     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:01     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:01     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:01     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:01     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:01     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:01     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:01     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:01     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:01     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:01     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:01     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:01     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:01     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:01     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:01     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:01     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:01     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:01     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:01     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:01     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:01     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:01     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:01 2025-01-13_19:41:01.948 [task-result-getter-2] WARN  o.a.s.s.TaskSetManager - Lost task 0.0 in stage 35.0 (TID 1411) (ee7845301a1d executor driver): java.util.NoSuchElementException: None.get
2025-01-13 22:41:01     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:01     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:01     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:01     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:01     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:01     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:01     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:01     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:01     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:01     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:01     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:01     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:01     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:01     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:01     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:01     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:01     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:01     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:01     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:01     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:01     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:01     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:01     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:01     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:01     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:01     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:01     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:01     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:01     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:01     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:01     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:01 
2025-01-13 22:41:01 2025-01-13_19:41:01.950 [task-result-getter-2] ERROR o.a.s.s.TaskSetManager - Task 0 in stage 35.0 failed 1 times; aborting job
2025-01-13 22:41:02 2025-01-13_19:41:02.095 [toFHIR-akka.actor.default-dispatcher-16] ERROR i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (FAILURE) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'observation.age_category'!
2025-01-13 22:41:02     # of Invalid Rows:      0
2025-01-13 22:41:02     # of Not Mapped:        0
2025-01-13 22:41:02     # of Failed writes:     0
2025-01-13 22:41:02     # of Written FHIR resources:    0
2025-01-13 22:41:02 org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 35.0 failed 1 times, most recent failure: Lost task 0.0 in stage 35.0 (TID 1411) (ee7845301a1d executor driver): java.util.NoSuchElementException: None.get
2025-01-13 22:41:02     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:02     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:02     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:02     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:02     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:02     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:02     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:02     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:02     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:02     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:02     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:02     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:02     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:02     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:02     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:02     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:02     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:02     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:02     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:02     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:02     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:02     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:02     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:02     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:02     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:02     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:02     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:02     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:02     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:02     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:02     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:02 
2025-01-13 22:41:02 Driver stacktrace:
2025-01-13 22:41:02     at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
2025-01-13 22:41:02     at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
2025-01-13 22:41:02     at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
2025-01-13 22:41:02     at scala.collection.immutable.List.foreach(List.scala:333)
2025-01-13 22:41:02     at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
2025-01-13 22:41:02     at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
2025-01-13 22:41:02     at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
2025-01-13 22:41:02     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:02     at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
2025-01-13 22:41:02     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
2025-01-13 22:41:02     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
2025-01-13 22:41:02     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
2025-01-13 22:41:02     at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
2025-01-13 22:41:02     at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
2025-01-13 22:41:02     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
2025-01-13 22:41:02     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
2025-01-13 22:41:02     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
2025-01-13 22:41:02     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
2025-01-13 22:41:02     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)
2025-01-13 22:41:02     at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
2025-01-13 22:41:02     at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
2025-01-13 22:41:02     at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
2025-01-13 22:41:02     at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)
2025-01-13 22:41:02     at org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3514)
2025-01-13 22:41:02     at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
2025-01-13 22:41:02     at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4309)
2025-01-13 22:41:02     at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
2025-01-13 22:41:02     at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
2025-01-13 22:41:02     at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
2025-01-13 22:41:02     at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-01-13 22:41:02     at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
2025-01-13 22:41:02     at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4307)
2025-01-13 22:41:02     at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3514)
2025-01-13 22:41:02     at io.tofhir.engine.data.write.FhirRepositoryWriter.write(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:02     at io.tofhir.engine.data.write.SinkHandler$.writeMappingResult(SinkHandler.scala:37)
2025-01-13 22:41:02     at io.tofhir.engine.mapping.job.FhirMappingJobManager.$anonfun$readSourceExecuteAndWriteInChunks$1(FhirMappingJobManager.scala:326)
2025-01-13 22:41:02     at io.tofhir.engine.mapping.job.FhirMappingJobManager.$anonfun$readSourceExecuteAndWriteInChunks$1$adapted(FhirMappingJobManager.scala:326)
2025-01-13 22:41:02     at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
2025-01-13 22:41:02     at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:64)
2025-01-13 22:41:02     at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:101)
2025-01-13 22:41:02     at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
2025-01-13 22:41:02     at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:94)
2025-01-13 22:41:02     at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:101)
2025-01-13 22:41:02     at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2025-01-13 22:41:02     at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2025-01-13 22:41:02     at java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)
2025-01-13 22:41:02     at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source)
2025-01-13 22:41:02     at java.base/java.util.concurrent.ForkJoinPool.scan(Unknown Source)
2025-01-13 22:41:02     at java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
2025-01-13 22:41:02     at java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)
2025-01-13 22:41:02 Caused by: java.util.NoSuchElementException: None.get
2025-01-13 22:41:02     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:02     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:02     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:02     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:02     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:02     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:02     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:02     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:02     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:02     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:02     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:02     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:02     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:02     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:02     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:02     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:02     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:02     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:02     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:02     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:02     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:02     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:02     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:02     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:02     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:02     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:02     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:02     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:02     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:02     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:02     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:02 2025-01-13_19:41:02.097 [toFHIR-akka.actor.default-dispatcher-16] INFO  i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (STARTED) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'karyotypic_sex'!
2025-01-13 22:41:02 
2025-01-13 22:41:02 2025-01-13_19:41:02.097 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Reading source data for mapping karyotypic_sex within mapping job rarelink-cdm ...
2025-01-13 22:41:03 2025-01-13_19:41:03.096 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - 5 records read for mapping karyotypic_sex within mapping job rarelink-cdm ...
2025-01-13 22:41:03 2025-01-13_19:41:03.108 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Executing the mapping karyotypic_sex within job rarelink-cdm ...
2025-01-13 22:41:03 2025-01-13_19:41:03.250 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.d.w.FhirRepositoryWriter - Created FHIR resources will be written to the given FHIR repository URL:http://172.21.0.2:8080/fhir
2025-01-13 22:41:03 2025-01-13_19:41:03.593 [Executor task launch worker for task 0.0 in stage 41.0 (TID 1416)] DEBUG i.t.e.d.w.FhirRepositoryWriter - Batch Update request will be sent to the FHIR repository for 5 resources.
2025-01-13 22:41:03 2025-01-13_19:41:03.654 [Executor task launch worker for task 0.0 in stage 41.0 (TID 1416)] ERROR o.a.s.e.Executor - Exception in task 0.0 in stage 41.0 (TID 1416)
2025-01-13 22:41:03 java.util.NoSuchElementException: None.get
2025-01-13 22:41:03     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:03     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:03     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:03     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:03     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:03     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:03     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:03     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:03     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:03     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:03     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:03     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:03     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:03     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:03     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:03     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:03     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:03     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:03     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:03     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:03     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:03     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:03 2025-01-13_19:41:03.669 [task-result-getter-3] WARN  o.a.s.s.TaskSetManager - Lost task 0.0 in stage 41.0 (TID 1416) (ee7845301a1d executor driver): java.util.NoSuchElementException: None.get
2025-01-13 22:41:03     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:03     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:03     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:03     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:03     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:03     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:03     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:03     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:03     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:03     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:03     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:03     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:03     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:03     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:03     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:03     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:03     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:03     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:03     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:03     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:03     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:03     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:03 
2025-01-13 22:41:03 2025-01-13_19:41:03.671 [task-result-getter-3] ERROR o.a.s.s.TaskSetManager - Task 0 in stage 41.0 failed 1 times; aborting job
2025-01-13 22:41:03 2025-01-13_19:41:03.687 [toFHIR-akka.actor.default-dispatcher-16] ERROR i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (FAILURE) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'karyotypic_sex'!
2025-01-13 22:41:03     # of Invalid Rows:      0
2025-01-13 22:41:03     # of Not Mapped:        0
2025-01-13 22:41:03     # of Failed writes:     0
2025-01-13 22:41:03     # of Written FHIR resources:    0
2025-01-13 22:41:03 org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 41.0 failed 1 times, most recent failure: Lost task 0.0 in stage 41.0 (TID 1416) (ee7845301a1d executor driver): java.util.NoSuchElementException: None.get
2025-01-13 22:41:03     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:03     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:03     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:03     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:03     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:03     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:03     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:03     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:03     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:03     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:03     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:03     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:03     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:03     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:03     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:03     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:03     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:03     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:03     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:03     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:03     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:03     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:03 
2025-01-13 22:41:03 Driver stacktrace:
2025-01-13 22:41:03     at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
2025-01-13 22:41:03     at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
2025-01-13 22:41:03     at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
2025-01-13 22:41:03     at scala.collection.immutable.List.foreach(List.scala:333)
2025-01-13 22:41:03     at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
2025-01-13 22:41:03     at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
2025-01-13 22:41:03     at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
2025-01-13 22:41:03     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:03     at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
2025-01-13 22:41:03     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
2025-01-13 22:41:03     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
2025-01-13 22:41:03     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
2025-01-13 22:41:03     at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
2025-01-13 22:41:03     at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
2025-01-13 22:41:03     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
2025-01-13 22:41:03     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
2025-01-13 22:41:03     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
2025-01-13 22:41:03     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
2025-01-13 22:41:03     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)
2025-01-13 22:41:03     at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
2025-01-13 22:41:03     at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
2025-01-13 22:41:03     at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
2025-01-13 22:41:03     at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)
2025-01-13 22:41:03     at org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3514)
2025-01-13 22:41:03     at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
2025-01-13 22:41:03     at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4309)
2025-01-13 22:41:03     at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
2025-01-13 22:41:03     at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
2025-01-13 22:41:03     at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
2025-01-13 22:41:03     at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-01-13 22:41:03     at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
2025-01-13 22:41:03     at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4307)
2025-01-13 22:41:03     at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3514)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.write(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.SinkHandler$.writeMappingResult(SinkHandler.scala:37)
2025-01-13 22:41:03     at io.tofhir.engine.mapping.job.FhirMappingJobManager.$anonfun$readSourceExecuteAndWriteInChunks$1(FhirMappingJobManager.scala:326)
2025-01-13 22:41:03     at io.tofhir.engine.mapping.job.FhirMappingJobManager.$anonfun$readSourceExecuteAndWriteInChunks$1$adapted(FhirMappingJobManager.scala:326)
2025-01-13 22:41:03     at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
2025-01-13 22:41:03     at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:64)
2025-01-13 22:41:03     at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:101)
2025-01-13 22:41:03     at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
2025-01-13 22:41:03     at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:94)
2025-01-13 22:41:03     at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:101)
2025-01-13 22:41:03     at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2025-01-13 22:41:03     at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2025-01-13 22:41:03     at java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)
2025-01-13 22:41:03     at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source)
2025-01-13 22:41:03     at java.base/java.util.concurrent.ForkJoinPool.scan(Unknown Source)
2025-01-13 22:41:03     at java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
2025-01-13 22:41:03     at java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)
2025-01-13 22:41:03 Caused by: java.util.NoSuchElementException: None.get
2025-01-13 22:41:03     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:03     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:03     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:03     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:03     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:03     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:03     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:03     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:03     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:03     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:03     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:03     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:03     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:03     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:03     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:03     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:03     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:03     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:03     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:03     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:03     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:03     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:03     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:03 2025-01-13_19:41:03.688 [toFHIR-akka.actor.default-dispatcher-16] INFO  i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (STARTED) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'ips.patient.vitalstatus'!
2025-01-13 22:41:03 
2025-01-13 22:41:03 2025-01-13_19:41:03.688 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Reading source data for mapping ips.patient.vitalstatus within mapping job rarelink-cdm ...
2025-01-13 22:41:10 2025-01-13_19:41:10.716 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - 5 records read for mapping ips.patient.vitalstatus within mapping job rarelink-cdm ...
2025-01-13 22:41:10 2025-01-13_19:41:10.728 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Executing the mapping ips.patient.vitalstatus within job rarelink-cdm ...
2025-01-13 22:41:11 2025-01-13_19:41:11.205 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.d.w.FhirRepositoryWriter - Created FHIR resources will be written to the given FHIR repository URL:http://172.21.0.2:8080/fhir
2025-01-13 22:41:13 2025-01-13_19:41:13.952 [Executor task launch worker for task 11.0 in stage 56.0 (TID 2032)] DEBUG i.t.e.d.w.FhirRepositoryWriter - Batch Update request will be sent to the FHIR repository for 1 resources.
2025-01-13 22:41:14 2025-01-13_19:41:14.171 [Executor task launch worker for task 11.0 in stage 56.0 (TID 2032)] DEBUG i.t.e.d.w.FhirRepositoryWriter - 1 FHIR resources were written to the FHIR repository successfully.
2025-01-13 22:41:14 2025-01-13_19:41:14.360 [Executor task launch worker for task 82.0 in stage 56.0 (TID 2103)] DEBUG i.t.e.d.w.FhirRepositoryWriter - Batch Update request will be sent to the FHIR repository for 1 resources.
2025-01-13 22:41:14 2025-01-13_19:41:14.401 [Executor task launch worker for task 88.0 in stage 56.0 (TID 2109)] DEBUG i.t.e.d.w.FhirRepositoryWriter - Batch Update request will be sent to the FHIR repository for 1 resources.
2025-01-13 22:41:14 2025-01-13_19:41:14.417 [Executor task launch worker for task 82.0 in stage 56.0 (TID 2103)] DEBUG i.t.e.d.w.FhirRepositoryWriter - 1 FHIR resources were written to the FHIR repository successfully.
2025-01-13 22:41:14 2025-01-13_19:41:14.474 [Executor task launch worker for task 88.0 in stage 56.0 (TID 2109)] DEBUG i.t.e.d.w.FhirRepositoryWriter - 1 FHIR resources were written to the FHIR repository successfully.
2025-01-13 22:41:14 2025-01-13_19:41:14.551 [Executor task launch worker for task 99.0 in stage 56.0 (TID 2120)] DEBUG i.t.e.d.w.FhirRepositoryWriter - Batch Update request will be sent to the FHIR repository for 1 resources.
2025-01-13 22:41:14 2025-01-13_19:41:14.583 [Executor task launch worker for task 99.0 in stage 56.0 (TID 2120)] DEBUG i.t.e.d.w.FhirRepositoryWriter - 1 FHIR resources were written to the FHIR repository successfully.
2025-01-13 22:41:15 2025-01-13_19:41:15.195 [Executor task launch worker for task 145.0 in stage 56.0 (TID 2166)] DEBUG i.t.e.d.w.FhirRepositoryWriter - Batch Update request will be sent to the FHIR repository for 1 resources.
2025-01-13 22:41:15 2025-01-13_19:41:15.390 [Executor task launch worker for task 145.0 in stage 56.0 (TID 2166)] DEBUG i.t.e.d.w.FhirRepositoryWriter - 1 FHIR resources were written to the FHIR repository successfully.
2025-01-13 22:41:22 2025-01-13_19:41:22.261 [toFHIR-akka.actor.default-dispatcher-16] INFO  i.t.e.e.l.ExecutionLogger$ - toFHIR chunk mapping result (SUCCESS) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'ips.patient.vitalstatus'!
2025-01-13 22:41:22     # of Invalid Rows:      0
2025-01-13 22:41:22     # of Not Mapped:        0
2025-01-13 22:41:22     # of Failed writes:     0
2025-01-13 22:41:22     # of Written FHIR resources:    5
2025-01-13 22:41:22 2025-01-13_19:41:22.344 [toFHIR-akka.actor.default-dispatcher-16] INFO  i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (SUCCESS) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'ips.patient.vitalstatus'!
2025-01-13 22:41:22     # of Invalid Rows:      0
2025-01-13 22:41:22     # of Not Mapped:        0
2025-01-13 22:41:22     # of Failed writes:     0
2025-01-13 22:41:22     # of Written FHIR resources:    5
2025-01-13 22:41:22 2025-01-13_19:41:22.346 [toFHIR-akka.actor.default-dispatcher-16] INFO  i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (STARTED) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'observation.length_of_gestation_at_birth'!
2025-01-13 22:41:22 
2025-01-13 22:41:22 2025-01-13_19:41:22.347 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Reading source data for mapping observation.length_of_gestation_at_birth within mapping job rarelink-cdm ...
2025-01-13 22:41:23 2025-01-13_19:41:23.899 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - 5 records read for mapping observation.length_of_gestation_at_birth within mapping job rarelink-cdm ...
2025-01-13 22:41:23 2025-01-13_19:41:23.907 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Executing the mapping observation.length_of_gestation_at_birth within job rarelink-cdm ...
2025-01-13 22:41:24 2025-01-13_19:41:24.128 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.d.w.FhirRepositoryWriter - Created FHIR resources will be written to the given FHIR repository URL:http://172.21.0.2:8080/fhir
2025-01-13 22:41:24 2025-01-13_19:41:24.499 [Executor task launch worker for task 0.0 in stage 77.0 (TID 2828)] DEBUG i.t.e.d.w.FhirRepositoryWriter - Batch Update request will be sent to the FHIR repository for 5 resources.
2025-01-13 22:41:24 2025-01-13_19:41:24.646 [Executor task launch worker for task 0.0 in stage 77.0 (TID 2828)] ERROR o.a.s.e.Executor - Exception in task 0.0 in stage 77.0 (TID 2828)
2025-01-13 22:41:24 java.util.NoSuchElementException: None.get
2025-01-13 22:41:24     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:24     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:24     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:24     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:24     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:24     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:24     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:24     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:24 ANTLR Tool version 4.9.3 used for code generation does not match the current runtime version 4.7ANTLR Runtime version 4.9.3 used for parser compilation does not match the current runtime version 4.7ANTLR Tool version 4.9.3 used for code generation does not match the current runtime version 4.7ANTLR Runtime version 4.9.3 used for parser compilation does not match the current runtime version 4.7line 1:32 mismatched input '[' expecting {'is', 'as', '$this', '$index', '$total', IDENTIFIER, QUOTEDIDENTIFIER}
2025-01-13 22:41:24 line 1:32 mismatched input '[' expecting {'is', 'as', '$this', '$index', '$total', IDENTIFIER, QUOTEDIDENTIFIER}
2025-01-13 22:41:24 line 1:32 mismatched input '[' expecting {'is', 'as', '$this', '$index', '$total', IDENTIFIER, QUOTEDIDENTIFIER}
2025-01-13 22:41:24 line 1:32 mismatched input '[' expecting {'is', 'as', '$this', '$index', '$total', IDENTIFIER, QUOTEDIDENTIFIER}
2025-01-13 22:41:24 line 1:32 mismatched input '[' expecting {'is', 'as', '$this', '$index', '$total', IDENTIFIER, QUOTEDIDENTIFIER}
2025-01-13 22:41:24 line 1:32 mismatched input '[' expecting {'is', 'as', '$this', '$index', '$total', IDENTIFIER, QUOTEDIDENTIFIER}
2025-01-13 22:41:24 line 1:32 mismatched input '[' expecting {'is', 'as', '$this', '$index', '$total', IDENTIFIER, QUOTEDIDENTIFIER}
2025-01-13 22:41:24 line 1:32 mismatched input '[' expecting {'is', 'as', '$this', '$index', '$total', IDENTIFIER, QUOTEDIDENTIFIER}
2025-01-13 22:41:24 line 1:32 mismatched input '[' expecting {'is', 'as', '$this', '$index', '$total', IDENTIFIER, QUOTEDIDENTIFIER}
2025-01-13 22:41:24 line 1:32 mismatched input '[' expecting {'is', 'as', '$this', '$index', '$total', IDENTIFIER, QUOTEDIDENTIFIER}
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:24     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:24     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:24     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:24     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:24     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:24     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:24     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:24     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:24     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:24     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:24     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:24     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:24     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:24     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:24 2025-01-13_19:41:24.655 [task-result-getter-0] WARN  o.a.s.s.TaskSetManager - Lost task 0.0 in stage 77.0 (TID 2828) (ee7845301a1d executor driver): java.util.NoSuchElementException: None.get
2025-01-13 22:41:24     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:24     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:24     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:24     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:24     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:24     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:24     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:24     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:24     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:24     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:24     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:24     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:24     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:24     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:24     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:24     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:24     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:24     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:24     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:24     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:24     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:24     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:24 
2025-01-13 22:41:24 2025-01-13_19:41:24.657 [task-result-getter-0] ERROR o.a.s.s.TaskSetManager - Task 0 in stage 77.0 failed 1 times; aborting job
2025-01-13 22:41:24 2025-01-13_19:41:24.674 [toFHIR-akka.actor.default-dispatcher-16] ERROR i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (FAILURE) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'observation.length_of_gestation_at_birth'!
2025-01-13 22:41:24     # of Invalid Rows:      0
2025-01-13 22:41:24     # of Not Mapped:        0
2025-01-13 22:41:24     # of Failed writes:     0
2025-01-13 22:41:24     # of Written FHIR resources:    0
2025-01-13 22:41:24 org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 77.0 failed 1 times, most recent failure: Lost task 0.0 in stage 77.0 (TID 2828) (ee7845301a1d executor driver): java.util.NoSuchElementException: None.get
2025-01-13 22:41:24     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:24     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:24     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:24     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:24     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:24     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:24     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:24     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:24     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:24     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:24     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:24     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:24     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:24     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:24     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:24     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:24     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:24     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:24     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:24     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:24     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:24     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:24 
2025-01-13 22:41:24 Driver stacktrace:
2025-01-13 22:41:24     at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
2025-01-13 22:41:24     at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
2025-01-13 22:41:24     at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
2025-01-13 22:41:24     at scala.collection.immutable.List.foreach(List.scala:333)
2025-01-13 22:41:24     at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
2025-01-13 22:41:24     at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
2025-01-13 22:41:24     at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
2025-01-13 22:41:24     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:24     at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
2025-01-13 22:41:24     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
2025-01-13 22:41:24     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
2025-01-13 22:41:24     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
2025-01-13 22:41:24     at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
2025-01-13 22:41:24     at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
2025-01-13 22:41:24     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
2025-01-13 22:41:24     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
2025-01-13 22:41:24     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
2025-01-13 22:41:24     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
2025-01-13 22:41:24     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)
2025-01-13 22:41:24     at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
2025-01-13 22:41:24     at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
2025-01-13 22:41:24     at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
2025-01-13 22:41:24     at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)
2025-01-13 22:41:24     at org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3514)
2025-01-13 22:41:24     at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
2025-01-13 22:41:24     at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4309)
2025-01-13 22:41:24     at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
2025-01-13 22:41:24     at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
2025-01-13 22:41:24     at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
2025-01-13 22:41:24     at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-01-13 22:41:24     at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
2025-01-13 22:41:24     at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4307)
2025-01-13 22:41:24     at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3514)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.write(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.SinkHandler$.writeMappingResult(SinkHandler.scala:37)
2025-01-13 22:41:24     at io.tofhir.engine.mapping.job.FhirMappingJobManager.$anonfun$readSourceExecuteAndWriteInChunks$1(FhirMappingJobManager.scala:326)
2025-01-13 22:41:24     at io.tofhir.engine.mapping.job.FhirMappingJobManager.$anonfun$readSourceExecuteAndWriteInChunks$1$adapted(FhirMappingJobManager.scala:326)
2025-01-13 22:41:24     at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
2025-01-13 22:41:24     at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:64)
2025-01-13 22:41:24     at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:101)
2025-01-13 22:41:24     at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
2025-01-13 22:41:24     at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:94)
2025-01-13 22:41:24     at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:101)
2025-01-13 22:41:24     at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2025-01-13 22:41:24     at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2025-01-13 22:41:24     at java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)
2025-01-13 22:41:24     at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source)
2025-01-13 22:41:24     at java.base/java.util.concurrent.ForkJoinPool.scan(Unknown Source)
2025-01-13 22:41:24     at java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
2025-01-13 22:41:24     at java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)
2025-01-13 22:41:24 Caused by: java.util.NoSuchElementException: None.get
2025-01-13 22:41:24     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:24     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:24     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:24     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:24     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:24     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:24     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:24     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:24     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:24     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:24     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:24     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:24     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:24     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:24     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:24     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:24     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:24     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:24     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:24     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:24     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:24     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:24     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:24 2025-01-13_19:41:24.678 [toFHIR-akka.actor.default-dispatcher-16] INFO  i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (STARTED) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'ips.condition.undiagnosed_rd_case'!
2025-01-13 22:41:24 
2025-01-13 22:41:24 2025-01-13_19:41:24.679 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Reading source data for mapping ips.condition.undiagnosed_rd_case within mapping job rarelink-cdm ...
2025-01-13 22:41:25 2025-01-13_19:41:25.652 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - 5 records read for mapping ips.condition.undiagnosed_rd_case within mapping job rarelink-cdm ...
2025-01-13 22:41:25 2025-01-13_19:41:25.654 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Executing the mapping ips.condition.undiagnosed_rd_case within job rarelink-cdm ...
2025-01-13 22:41:25 2025-01-13_19:41:25.797 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.d.w.FhirRepositoryWriter - Created FHIR resources will be written to the given FHIR repository URL:http://172.21.0.2:8080/fhir
2025-01-13 22:41:26 2025-01-13_19:41:26.055 [Executor task launch worker for task 0.0 in stage 83.0 (TID 2833)] DEBUG i.t.e.d.w.FhirRepositoryWriter - Batch Update request will be sent to the FHIR repository for 3 resources.
2025-01-13 22:41:26 2025-01-13_19:41:26.099 [Executor task launch worker for task 0.0 in stage 83.0 (TID 2833)] ERROR o.a.s.e.Executor - Exception in task 0.0 in stage 83.0 (TID 2833)
2025-01-13 22:41:26 java.util.NoSuchElementException: None.get
2025-01-13 22:41:26     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:26     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:26     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:26     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:26     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:26     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:26     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:26     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:26     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:26     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:26     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:26     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:26     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:26     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:26     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:26     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:26     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:26     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:26     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:26     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:26     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:26     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:26 2025-01-13_19:41:26.101 [task-result-getter-1] WARN  o.a.s.s.TaskSetManager - Lost task 0.0 in stage 83.0 (TID 2833) (ee7845301a1d executor driver): java.util.NoSuchElementException: None.get
2025-01-13 22:41:26     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:26     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:26     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:26     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:26     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:26     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:26     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:26     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:26     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:26     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:26     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:26     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:26     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:26     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:26     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:26     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:26     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:26     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:26     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:26     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:26     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:26     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:26 
2025-01-13 22:41:26 2025-01-13_19:41:26.101 [task-result-getter-1] ERROR o.a.s.s.TaskSetManager - Task 0 in stage 83.0 failed 1 times; aborting job
2025-01-13 22:41:26 2025-01-13_19:41:26.109 [toFHIR-akka.actor.default-dispatcher-16] ERROR i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (FAILURE) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'ips.condition.undiagnosed_rd_case'!
2025-01-13 22:41:26     # of Invalid Rows:      0
2025-01-13 22:41:26     # of Not Mapped:        0
2025-01-13 22:41:26     # of Failed writes:     0
2025-01-13 22:41:26     # of Written FHIR resources:    0
2025-01-13 22:41:26 org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 83.0 failed 1 times, most recent failure: Lost task 0.0 in stage 83.0 (TID 2833) (ee7845301a1d executor driver): java.util.NoSuchElementException: None.get
2025-01-13 22:41:26     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:26     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:26     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:26     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:26     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:26     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:26     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:26     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:26     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:26     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:26     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:26     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:26     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:26     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:26     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:26     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:26     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:26     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:26     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:26     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:26     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:26     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:26 
2025-01-13 22:41:26 Driver stacktrace:
2025-01-13 22:41:26     at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
2025-01-13 22:41:26     at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
2025-01-13 22:41:26     at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
2025-01-13 22:41:26     at scala.collection.immutable.List.foreach(List.scala:333)
2025-01-13 22:41:26     at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
2025-01-13 22:41:26     at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
2025-01-13 22:41:26     at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
2025-01-13 22:41:26     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:26     at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
2025-01-13 22:41:26     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
2025-01-13 22:41:26     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
2025-01-13 22:41:26     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
2025-01-13 22:41:26     at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
2025-01-13 22:41:26     at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
2025-01-13 22:41:26     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
2025-01-13 22:41:26     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
2025-01-13 22:41:26     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
2025-01-13 22:41:26     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
2025-01-13 22:41:26     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)
2025-01-13 22:41:26     at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
2025-01-13 22:41:26     at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
2025-01-13 22:41:26     at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
2025-01-13 22:41:26     at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)
2025-01-13 22:41:26     at org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3514)
2025-01-13 22:41:26     at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
2025-01-13 22:41:26     at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4309)
2025-01-13 22:41:26     at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
2025-01-13 22:41:26     at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
2025-01-13 22:41:26     at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
2025-01-13 22:41:26     at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-01-13 22:41:26     at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
2025-01-13 22:41:26     at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4307)
2025-01-13 22:41:26     at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3514)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.write(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.SinkHandler$.writeMappingResult(SinkHandler.scala:37)
2025-01-13 22:41:26     at io.tofhir.engine.mapping.job.FhirMappingJobManager.$anonfun$readSourceExecuteAndWriteInChunks$1(FhirMappingJobManager.scala:326)
2025-01-13 22:41:26     at io.tofhir.engine.mapping.job.FhirMappingJobManager.$anonfun$readSourceExecuteAndWriteInChunks$1$adapted(FhirMappingJobManager.scala:326)
2025-01-13 22:41:26     at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
2025-01-13 22:41:26     at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:64)
2025-01-13 22:41:26     at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:101)
2025-01-13 22:41:26     at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
2025-01-13 22:41:26     at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:94)
2025-01-13 22:41:26     at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:101)
2025-01-13 22:41:26     at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2025-01-13 22:41:26     at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2025-01-13 22:41:26     at java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)
2025-01-13 22:41:26     at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source)
2025-01-13 22:41:26     at java.base/java.util.concurrent.ForkJoinPool.scan(Unknown Source)
2025-01-13 22:41:26     at java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
2025-01-13 22:41:26     at java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)
2025-01-13 22:41:26 Caused by: java.util.NoSuchElementException: None.get
2025-01-13 22:41:26     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:26     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:26     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:26     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:26     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:26     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:26     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:26     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:26     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:26     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:26     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:26     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:26     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:26     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:26     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:26     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:26     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:26     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:26     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:26     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:26     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:26     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:26     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:26 2025-01-13_19:41:26.110 [toFHIR-akka.actor.default-dispatcher-16] INFO  i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (STARTED) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'encounter.care_pathway'!
2025-01-13 22:41:26 
2025-01-13 22:41:26 2025-01-13_19:41:26.111 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Reading source data for mapping encounter.care_pathway within mapping job rarelink-cdm ...
2025-01-13 22:41:27 2025-01-13_19:41:27.279 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - 8 records read for mapping encounter.care_pathway within mapping job rarelink-cdm ...
2025-01-13 22:41:27 2025-01-13_19:41:27.281 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Executing the mapping encounter.care_pathway within job rarelink-cdm ...
2025-01-13 22:41:27 2025-01-13_19:41:27.372 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.d.w.FhirRepositoryWriter - Created FHIR resources will be written to the given FHIR repository URL:http://172.21.0.2:8080/fhir
2025-01-13 22:41:27 2025-01-13_19:41:27.641 [Executor task launch worker for task 0.0 in stage 89.0 (TID 2838)] DEBUG i.t.e.d.w.FhirRepositoryWriter - Batch Update request will be sent to the FHIR repository for 8 resources.
2025-01-13 22:41:27 2025-01-13_19:41:27.714 [Executor task launch worker for task 0.0 in stage 89.0 (TID 2838)] DEBUG i.t.e.d.w.FhirRepositoryWriter - 8 FHIR resources were written to the FHIR repository successfully.
2025-01-13 22:41:28 2025-01-13_19:41:28.915 [toFHIR-akka.actor.default-dispatcher-16] INFO  i.t.e.e.l.ExecutionLogger$ - toFHIR chunk mapping result (SUCCESS) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'encounter.care_pathway'!
2025-01-13 22:41:28     # of Invalid Rows:      0
2025-01-13 22:41:28     # of Not Mapped:        0
2025-01-13 22:41:27 line 1:64 missing ')' at '<EOF>'
2025-01-13 22:41:27 line 1:64 missing ')' at '<EOF>'
2025-01-13 22:41:27 line 1:64 missing ')' at '<EOF>'
2025-01-13 22:41:27 line 1:64 missing ')' at '<EOF>'
2025-01-13 22:41:27 line 1:64 missing ')' at '<EOF>'
2025-01-13 22:41:27 line 1:64 missing ')' at '<EOF>'
2025-01-13 22:41:27 line 1:64 missing ')' at '<EOF>'
2025-01-13 22:41:27 line 1:64 missing ')' at '<EOF>'
2025-01-13 22:41:28     # of Failed writes:     0
2025-01-13 22:41:28     # of Written FHIR resources:    8
2025-01-13 22:41:28 2025-01-13_19:41:28.929 [toFHIR-akka.actor.default-dispatcher-16] INFO  i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (SUCCESS) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'encounter.care_pathway'!
2025-01-13 22:41:28     # of Invalid Rows:      0
2025-01-13 22:41:28     # of Not Mapped:        0
2025-01-13 22:41:28     # of Failed writes:     0
2025-01-13 22:41:28     # of Written FHIR resources:    8
2025-01-13 22:41:28 2025-01-13_19:41:28.931 [toFHIR-akka.actor.default-dispatcher-16] INFO  i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (STARTED) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'ips.condition'!
2025-01-13 22:41:28 
2025-01-13 22:41:28 2025-01-13_19:41:28.932 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Reading source data for mapping ips.condition within mapping job rarelink-cdm ...
2025-01-13 22:41:29 2025-01-13_19:41:29.958 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - 13 records read for mapping ips.condition within mapping job rarelink-cdm ...
2025-01-13 22:41:29 2025-01-13_19:41:29.958 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Executing the mapping ips.condition within job rarelink-cdm ...
2025-01-13 22:41:30 2025-01-13_19:41:30.043 [toFHIR-akka.actor.default-dispatcher-16] WARN  o.a.s.s.c.u.SparkStringUtils - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
2025-01-13 22:41:30 2025-01-13_19:41:30.069 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.d.w.FhirRepositoryWriter - Created FHIR resources will be written to the given FHIR repository URL:http://172.21.0.2:8080/fhir
2025-01-13 22:41:30 2025-01-13_19:41:30.476 [Executor task launch worker for task 0.0 in stage 104.0 (TID 2849)] DEBUG i.t.e.d.w.FhirRepositoryWriter - Batch Update request will be sent to the FHIR repository for 13 resources.
2025-01-13 22:41:30 2025-01-13_19:41:30.577 [Executor task launch worker for task 0.0 in stage 104.0 (TID 2849)] ERROR o.a.s.e.Executor - Exception in task 0.0 in stage 104.0 (TID 2849)
2025-01-13 22:41:30 java.util.NoSuchElementException: None.get
2025-01-13 22:41:30     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:30     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:30     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:30     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:30     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:30     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:30     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:30     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:30     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:30     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:30     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:30     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:30     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:30     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:30     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:30     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:30     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:30     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:30     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:30     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:30     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:30     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:30 2025-01-13_19:41:30.582 [task-result-getter-1] WARN  o.a.s.s.TaskSetManager - Lost task 0.0 in stage 104.0 (TID 2849) (ee7845301a1d executor driver): java.util.NoSuchElementException: None.get
2025-01-13 22:41:30     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:30     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:30     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:30     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:30     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:30     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:30     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:30     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:30     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:30     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:30     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:30     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:30     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:30     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:30     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:30     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:30     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:30     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:30     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:30     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:30     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:30     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:30 
2025-01-13 22:41:30 2025-01-13_19:41:30.582 [task-result-getter-1] ERROR o.a.s.s.TaskSetManager - Task 0 in stage 104.0 failed 1 times; aborting job
2025-01-13 22:41:30 2025-01-13_19:41:30.590 [toFHIR-akka.actor.default-dispatcher-16] ERROR i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (FAILURE) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'ips.condition'!
2025-01-13 22:41:30     # of Invalid Rows:      0
2025-01-13 22:41:30     # of Not Mapped:        0
2025-01-13 22:41:30     # of Failed writes:     0
2025-01-13 22:41:30     # of Written FHIR resources:    0
2025-01-13 22:41:30 org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 104.0 failed 1 times, most recent failure: Lost task 0.0 in stage 104.0 (TID 2849) (ee7845301a1d executor driver): java.util.NoSuchElementException: None.get
2025-01-13 22:41:30     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:30     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:30     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:30     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:30     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:30     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:30     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:30     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:30     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:30     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:30     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:30     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:30     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:30     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:30     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:30     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:30     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:30     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:30     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:30     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:30     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:30     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:30 
2025-01-13 22:41:30 Driver stacktrace:
2025-01-13 22:41:30     at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
2025-01-13 22:41:30     at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
2025-01-13 22:41:30     at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
2025-01-13 22:41:30     at scala.collection.immutable.List.foreach(List.scala:333)
2025-01-13 22:41:30     at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
2025-01-13 22:41:30     at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
2025-01-13 22:41:30     at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
2025-01-13 22:41:30     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:30     at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
2025-01-13 22:41:30     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
2025-01-13 22:41:30     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
2025-01-13 22:41:30     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
2025-01-13 22:41:30     at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
2025-01-13 22:41:30     at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
2025-01-13 22:41:30     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
2025-01-13 22:41:30     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
2025-01-13 22:41:30     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
2025-01-13 22:41:30     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
2025-01-13 22:41:30     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)
2025-01-13 22:41:30     at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
2025-01-13 22:41:30     at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
2025-01-13 22:41:30     at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
2025-01-13 22:41:30     at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)
2025-01-13 22:41:30     at org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3514)
2025-01-13 22:41:30     at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
2025-01-13 22:41:30     at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4309)
2025-01-13 22:41:30     at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
2025-01-13 22:41:30     at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
2025-01-13 22:41:30     at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
2025-01-13 22:41:30     at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-01-13 22:41:30     at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
2025-01-13 22:41:30     at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4307)
2025-01-13 22:41:30     at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3514)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.write(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.SinkHandler$.writeMappingResult(SinkHandler.scala:37)
2025-01-13 22:41:30     at io.tofhir.engine.mapping.job.FhirMappingJobManager.$anonfun$readSourceExecuteAndWriteInChunks$1(FhirMappingJobManager.scala:326)
2025-01-13 22:41:30     at io.tofhir.engine.mapping.job.FhirMappingJobManager.$anonfun$readSourceExecuteAndWriteInChunks$1$adapted(FhirMappingJobManager.scala:326)
2025-01-13 22:41:30     at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
2025-01-13 22:41:30     at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:64)
2025-01-13 22:41:30     at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:101)
2025-01-13 22:41:30     at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
2025-01-13 22:41:30     at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:94)
2025-01-13 22:41:30     at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:101)
2025-01-13 22:41:30     at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2025-01-13 22:41:30     at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2025-01-13 22:41:30     at java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)
2025-01-13 22:41:30     at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source)
2025-01-13 22:41:30     at java.base/java.util.concurrent.ForkJoinPool.scan(Unknown Source)
2025-01-13 22:41:30     at java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
2025-01-13 22:41:30     at java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)
2025-01-13 22:41:30 Caused by: java.util.NoSuchElementException: None.get
2025-01-13 22:41:30     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:30     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:30     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:30     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:30     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:30     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:30     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:30     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:30     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:30     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:30     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:30     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:30     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:30     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:30     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:30     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:30     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:30     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:30     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:30     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:30     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:30     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:30     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:30 2025-01-13_19:41:30.596 [toFHIR-akka.actor.default-dispatcher-16] INFO  i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (STARTED) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'observation.genetic_findings'!
2025-01-13 22:41:30 
2025-01-13 22:41:30 2025-01-13_19:41:30.596 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Reading source data for mapping observation.genetic_findings within mapping job rarelink-cdm ...
2025-01-13 22:41:31 2025-01-13_19:41:31.524 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - 7 records read for mapping observation.genetic_findings within mapping job rarelink-cdm ...
2025-01-13 22:41:31 2025-01-13_19:41:31.525 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Executing the mapping observation.genetic_findings within job rarelink-cdm ...
2025-01-13 22:41:31 2025-01-13_19:41:31.604 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.d.w.FhirRepositoryWriter - Created FHIR resources will be written to the given FHIR repository URL:http://172.21.0.2:8080/fhir
2025-01-13 22:41:31 2025-01-13_19:41:31.947 [Executor task launch worker for task 0.0 in stage 110.0 (TID 2854)] DEBUG i.t.e.d.w.FhirRepositoryWriter - Batch Update request will be sent to the FHIR repository for 7 resources.
2025-01-13 22:41:32 2025-01-13_19:41:32.002 [Executor task launch worker for task 0.0 in stage 110.0 (TID 2854)] ERROR i.t.e.d.w.FhirRepositoryWriter - Firely did not return an expression indicating the location of the OutcomeIssue: {"severity":"error","code":"processing","diagnostics":"HAPI-0450: Failed to parse request body as JSON resource. Error was: HAPI-1755: String does not appear to be valid XML/XHTML (error is \"Unexpected character '\\' (code 92) in start tag Expected a quote\n at [row,col {unknown-source}]: [1,12]\"): <div xmlns=\\\"http://www.w3.org/1999/xhtml\\\">  <p><strong>RareLink Observation</strong></p>  <p>This observation is generated as part of the RareLink REDCap project and its Common Data Model (CDM). It provides a comprehensive framework around REDCap harmonising and linking international rare disease registries using FHIR, Phenopackets, and international Ontologies and Terminologies. This resource contains structured data and extensions to ensure interoperability and adherence to international standards.</p>  <ul>    <li><strong>Status:</strong> Registered</li>    <li><strong>Category:</strong> Phenotypic Feature</li>    <li><strong>Code:</strong> Human Phenotype Ontology (HPO)</li>    <li><strong>Extensions:</strong> Includes additional metadata for enhanced interpretation</li>  </ul>  <p>For more details, visit the <a href=\\\"https://github.com/BIH-CEI/rarelink\\\">RareLink GitHub Repository</a> or the <a href=\\\"https://rarelink.readthedocs.io/en/latest/index.html\\\">documentation</a>.</p></div>","expression":[]}
2025-01-13 22:41:32 2025-01-13_19:41:32.362 [toFHIR-akka.actor.default-dispatcher-16] INFO  i.t.e.e.l.ExecutionLogger$ - toFHIR chunk mapping result (FAILURE) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'observation.genetic_findings'!
2025-01-13 22:41:32     # of Invalid Rows:      0
2025-01-13 22:41:32     # of Not Mapped:        0
2025-01-13 22:41:32     # of Failed writes:     7
2025-01-13 22:41:32     # of Written FHIR resources:    0
2025-01-13 22:41:32 2025-01-13_19:41:32.366 [toFHIR-akka.actor.default-dispatcher-16] WARN  i.t.e.d.w.SinkHandler$ - Mapping failure (invalid_resource) for job 'rarelink-cdm' and mappingTask 'observation.genetic_findings' within expression 'observation.genetic_findings' execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3'!
2025-01-13 22:41:32     Source: {"mainSource":{"record_id":"102","genetic_diagnosis_code":"mondo","snomedct_106221001_mondo":"MONDO:0006823","snomedct_106221001_omim_p":null,"ga4gh_progress_status":"ga4gh_in_progress","ga4gh_interp_status":"ga4gh_rejected","loinc_81304_8":"loinc_la26418-6","loinc_62374_4":"loinc_la26806-2","loinc_lp7824_8":null,"variant_expression":"chgvs","loinc_81290_9":null,"loinc_48004_6":"NM_001377304.1:c.-16G>A","loinc_48005_3":null,"variant_validation":"yes","loinc_48018_6":"HGNC:4238","loinc_53034_5":"loinc_la6705-3","loinc_53034_5_other":null,"loinc_48002_0":"loinc_la6684-0","loinc_48019_4":"loinc_la6692-3","loinc_48019_4_other":null,"loinc_53037_8":"loinc_la26332-9","ga4gh_therap_action":"ga4gh_not_actionable","loinc_93044_6":"loinc_la30201-0"}}
2025-01-13 22:41:32     Error: Resource is not a valid FHIR resource or conforming to the indicated profiles!
2025-01-13 22:41:32     Expression: 
2025-01-13 22:41:32 2025-01-13_19:41:32.366 [toFHIR-akka.actor.default-dispatcher-16] WARN  i.t.e.d.w.SinkHandler$ - Mapping failure (invalid_resource) for job 'rarelink-cdm' and mappingTask 'observation.genetic_findings' within expression 'observation.genetic_findings' execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3'!
2025-01-13 22:41:32     Source: {"mainSource":{"record_id":"105","genetic_diagnosis_code":"mondo","snomedct_106221001_mondo":"MONDO:0019499","snomedct_106221001_omim_p":null,"ga4gh_progress_status":"ga4gh_completed","ga4gh_interp_status":"ga4gh_candidate","loinc_81304_8":"loinc_la26404-6","loinc_62374_4":"loinc_la26806-2","loinc_lp7824_8":null,"variant_expression":"chgvs","loinc_81290_9":null,"loinc_48004_6":"NM_006883.2:c.-507G>C","loinc_48005_3":null,"variant_validation":"yes","loinc_48018_6":"HGNC:10853","loinc_53034_5":"loinc_la6706-1","loinc_53034_5_other":null,"loinc_48002_0":"loinc_la10429-1","loinc_48019_4":"loinc_la6686-5","loinc_48019_4_other":null,"loinc_53037_8":"loinc_la26333-7","ga4gh_therap_action":"ga4gh_not_actionable","loinc_93044_6":"loinc_la30201-0"}}
2025-01-13 22:41:32     Error: Resource is not a valid FHIR resource or conforming to the indicated profiles!
2025-01-13 22:41:32     Expression: 
2025-01-13 22:41:31 line 1:20 token recognition error at: '''
2025-01-13 22:41:31 line 1:20 token recognition error at: '''
2025-01-13 22:41:31 line 1:55 missing ')' at '<EOF>'
2025-01-13 22:41:31 line 1:20 token recognition error at: '''
2025-01-13 22:41:31 line 1:20 token recognition error at: '''
2025-01-13 22:41:31 line 1:55 missing ')' at '<EOF>'
2025-01-13 22:41:31 line 1:20 token recognition error at: '''
2025-01-13 22:41:31 line 1:20 token recognition error at: '''
2025-01-13 22:41:31 line 1:55 missing ')' at '<EOF>'
2025-01-13 22:41:31 line 1:20 token recognition error at: '''
2025-01-13 22:41:31 line 1:20 token recognition error at: '''
2025-01-13 22:41:31 line 1:55 missing ')' at '<EOF>'
2025-01-13 22:41:31 line 1:20 token recognition error at: '''
2025-01-13 22:41:31 line 1:20 token recognition error at: '''
2025-01-13 22:41:31 line 1:55 missing ')' at '<EOF>'
2025-01-13 22:41:31 line 1:20 token recognition error at: '''
2025-01-13 22:41:31 line 1:20 token recognition error at: '''
2025-01-13 22:41:31 line 1:55 missing ')' at '<EOF>'
2025-01-13 22:41:31 line 1:20 token recognition error at: '''
2025-01-13 22:41:31 line 1:20 token recognition error at: '''
2025-01-13 22:41:31 line 1:55 missing ')' at '<EOF>'
2025-01-13 22:41:32 2025-01-13_19:41:32.366 [toFHIR-akka.actor.default-dispatcher-16] WARN  i.t.e.d.w.SinkHandler$ - Mapping failure (invalid_resource) for job 'rarelink-cdm' and mappingTask 'observation.genetic_findings' within expression 'observation.genetic_findings' execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3'!
2025-01-13 22:41:32     Source: {"mainSource":{"record_id":"103","genetic_diagnosis_code":"mondo","snomedct_106221001_mondo":"MONDO:0010518","snomedct_106221001_omim_p":null,"ga4gh_progress_status":"ga4gh_in_progress","ga4gh_interp_status":"ga4gh_contributory","loinc_81304_8":"loinc_la26418-6","loinc_62374_4":"loinc_la26806-2","loinc_lp7824_8":null,"variant_expression":"chgvs","loinc_81290_9":null,"loinc_48004_6":"NM_000377.3:c.6T>C","loinc_48005_3":null,"variant_validation":"yes","loinc_48018_6":null,"loinc_53034_5":"loinc_la6705-3","loinc_53034_5_other":null,"loinc_48002_0":"loinc_la6684-0","loinc_48019_4":"loinc_la6692-3","loinc_48019_4_other":null,"loinc_53037_8":"loinc_la26332-9","ga4gh_therap_action":"ga4gh_not_actionable","loinc_93044_6":"loinc_la30201-0"}}
2025-01-13 22:41:32     Error: Resource is not a valid FHIR resource or conforming to the indicated profiles!
2025-01-13 22:41:32     Expression: 
2025-01-13 22:41:32 2025-01-13_19:41:32.366 [toFHIR-akka.actor.default-dispatcher-16] WARN  i.t.e.d.w.SinkHandler$ - Mapping failure (invalid_resource) for job 'rarelink-cdm' and mappingTask 'observation.genetic_findings' within expression 'observation.genetic_findings' execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3'!
2025-01-13 22:41:32     Source: {"mainSource":{"record_id":"104","genetic_diagnosis_code":"mondo","snomedct_106221001_mondo":"MONDO:0010518","snomedct_106221001_omim_p":null,"ga4gh_progress_status":"ga4gh_in_progress","ga4gh_interp_status":"ga4gh_contributory","loinc_81304_8":"loinc_la26418-6","loinc_62374_4":"loinc_la26806-2","loinc_lp7824_8":null,"variant_expression":"chgvs","loinc_81290_9":null,"loinc_48004_6":"NM_000377.3:c.6T>C","loinc_48005_3":null,"variant_validation":"yes","loinc_48018_6":null,"loinc_53034_5":"loinc_la6705-3","loinc_53034_5_other":null,"loinc_48002_0":"loinc_la6684-0","loinc_48019_4":"loinc_la6692-3","loinc_48019_4_other":null,"loinc_53037_8":"loinc_la26332-9","ga4gh_therap_action":"ga4gh_not_actionable","loinc_93044_6":"loinc_la30201-0"}}
2025-01-13 22:41:32     Error: Resource is not a valid FHIR resource or conforming to the indicated profiles!
2025-01-13 22:41:32     Expression: 
2025-01-13 22:41:32 2025-01-13_19:41:32.366 [toFHIR-akka.actor.default-dispatcher-16] WARN  i.t.e.d.w.SinkHandler$ - Mapping failure (invalid_resource) for job 'rarelink-cdm' and mappingTask 'observation.genetic_findings' within expression 'observation.genetic_findings' execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3'!
2025-01-13 22:41:32     Source: {"mainSource":{"record_id":"104","genetic_diagnosis_code":"mondo","snomedct_106221001_mondo":"MONDO:0010518","snomedct_106221001_omim_p":null,"ga4gh_progress_status":"ga4gh_unsolved","ga4gh_interp_status":"ga4gh_candidate","loinc_81304_8":"loinc_la26404-6","loinc_62374_4":"loinc_la26806-2","loinc_lp7824_8":null,"variant_expression":"ghgvs","loinc_81290_9":"NC_000023.11:g.48683512T>C","loinc_48004_6":null,"loinc_48005_3":null,"variant_validation":"yes","loinc_48018_6":"HGNC:12731","loinc_53034_5":"loinc_la6706-1","loinc_53034_5_other":null,"loinc_48002_0":"loinc_la18194-3","loinc_48019_4":"loinc_la6692-3","loinc_48019_4_other":null,"loinc_53037_8":"loinc_la26332-9","ga4gh_therap_action":"ga4gh_actionable","loinc_93044_6":"loinc_la30200-2"}}
2025-01-13 22:41:32     Error: Resource is not a valid FHIR resource or conforming to the indicated profiles!
2025-01-13 22:41:32     Expression: 
2025-01-13 22:41:32 2025-01-13_19:41:32.366 [toFHIR-akka.actor.default-dispatcher-16] WARN  i.t.e.d.w.SinkHandler$ - Mapping failure (invalid_resource) for job 'rarelink-cdm' and mappingTask 'observation.genetic_findings' within expression 'observation.genetic_findings' execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3'!
2025-01-13 22:41:32     Source: {"mainSource":{"record_id":"101","genetic_diagnosis_code":"omim","snomedct_106221001_mondo":null,"snomedct_106221001_omim_p":"241600","ga4gh_progress_status":"ga4gh_completed","ga4gh_interp_status":"ga4gh_candidate","loinc_81304_8":"loinc_la26418-6","loinc_62374_4":"loinc_la26806-2","loinc_lp7824_8":null,"variant_expression":"chgvs","loinc_81290_9":null,"loinc_48004_6":"NM_006883.2:c.-507G>C","loinc_48005_3":null,"variant_validation":"yes","loinc_48018_6":"HGNC:10853","loinc_53034_5":"loinc_la6706-1","loinc_53034_5_other":null,"loinc_48002_0":"loinc_la10429-1","loinc_48019_4":"loinc_la6686-5","loinc_48019_4_other":null,"loinc_53037_8":"loinc_la26333-7","ga4gh_therap_action":"ga4gh_not_actionable","loinc_93044_6":"loinc_la30201-0"}}
2025-01-13 22:41:32     Error: Resource is not a valid FHIR resource or conforming to the indicated profiles!
2025-01-13 22:41:32     Expression: 
2025-01-13 22:41:32 2025-01-13_19:41:32.366 [toFHIR-akka.actor.default-dispatcher-16] WARN  i.t.e.d.w.SinkHandler$ - Mapping failure (invalid_resource) for job 'rarelink-cdm' and mappingTask 'observation.genetic_findings' within expression 'observation.genetic_findings' execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3'!
2025-01-13 22:41:32     Source: {"mainSource":{"record_id":"103","genetic_diagnosis_code":"mondo","snomedct_106221001_mondo":"MONDO:0010518","snomedct_106221001_omim_p":null,"ga4gh_progress_status":"ga4gh_unsolved","ga4gh_interp_status":"ga4gh_candidate","loinc_81304_8":"loinc_la26404-6","loinc_62374_4":"loinc_la26806-2","loinc_lp7824_8":null,"variant_expression":"ghgvs","loinc_81290_9":"NC_000023.11:g.48683512T>C","loinc_48004_6":null,"loinc_48005_3":null,"variant_validation":"yes","loinc_48018_6":"HGNC:12731","loinc_53034_5":"loinc_la6706-1","loinc_53034_5_other":null,"loinc_48002_0":"loinc_la18194-3","loinc_48019_4":"loinc_la6692-3","loinc_48019_4_other":null,"loinc_53037_8":"loinc_la26332-9","ga4gh_therap_action":"ga4gh_actionable","loinc_93044_6":"loinc_la30200-2"}}
2025-01-13 22:41:32     Error: Resource is not a valid FHIR resource or conforming to the indicated profiles!
2025-01-13 22:41:32     Expression: 
2025-01-13 22:41:32 2025-01-13_19:41:32.371 [toFHIR-akka.actor.default-dispatcher-16] INFO  i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (FAILURE) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'observation.genetic_findings'!
2025-01-13 22:41:32     # of Invalid Rows:      0
2025-01-13 22:41:32     # of Not Mapped:        0
2025-01-13 22:41:32     # of Failed writes:     7
2025-01-13 22:41:32     # of Written FHIR resources:    0
2025-01-13 22:41:32 2025-01-13_19:41:32.372 [toFHIR-akka.actor.default-dispatcher-16] INFO  i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (STARTED) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'observation.phenotypic_fetaure'!
2025-01-13 22:41:32 
2025-01-13 22:41:32 2025-01-13_19:41:32.373 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Reading source data for mapping observation.phenotypic_fetaure within mapping job rarelink-cdm ...
2025-01-13 22:41:33 2025-01-13_19:41:33.721 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - 20 records read for mapping observation.phenotypic_fetaure within mapping job rarelink-cdm ...
2025-01-13 22:41:33 2025-01-13_19:41:33.735 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Executing the mapping observation.phenotypic_fetaure within job rarelink-cdm ...
2025-01-13 22:41:33 2025-01-13_19:41:33.869 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.d.w.FhirRepositoryWriter - Created FHIR resources will be written to the given FHIR repository URL:http://172.21.0.2:8080/fhir
2025-01-13 22:41:34 2025-01-13_19:41:34.295 [Executor task launch worker for task 0.0 in stage 125.0 (TID 2865)] DEBUG i.t.e.d.w.FhirRepositoryWriter - Batch Update request will be sent to the FHIR repository for 20 resources.
2025-01-13 22:41:34 2025-01-13_19:41:34.506 [Executor task launch worker for task 0.0 in stage 125.0 (TID 2865)] ERROR o.a.s.e.Executor - Exception in task 0.0 in stage 125.0 (TID 2865)
2025-01-13 22:41:34 java.util.NoSuchElementException: None.get
2025-01-13 22:41:34     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:34     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:34     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:34     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:34     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:34     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:34     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:34     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:34     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:34     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:34     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:34     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:34     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:34     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:34     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:34     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:34     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:34     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:34     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:34     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:34     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:34     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:34 2025-01-13_19:41:34.513 [task-result-getter-1] WARN  o.a.s.s.TaskSetManager - Lost task 0.0 in stage 125.0 (TID 2865) (ee7845301a1d executor driver): java.util.NoSuchElementException: None.get
2025-01-13 22:41:34     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:34     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:34     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:34     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:34     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:34     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:34     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:34     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:34     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:34     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:34     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:34     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:34     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:34     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:34     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:34     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:34     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:34     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:34     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:34     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:34     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:34     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:34 
2025-01-13 22:41:34 2025-01-13_19:41:34.514 [task-result-getter-1] ERROR o.a.s.s.TaskSetManager - Task 0 in stage 125.0 failed 1 times; aborting job
2025-01-13 22:41:34 2025-01-13_19:41:34.522 [toFHIR-akka.actor.default-dispatcher-16] ERROR i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (FAILURE) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'observation.phenotypic_fetaure'!
2025-01-13 22:41:34     # of Invalid Rows:      0
2025-01-13 22:41:34     # of Not Mapped:        0
2025-01-13 22:41:34     # of Failed writes:     0
2025-01-13 22:41:34     # of Written FHIR resources:    0
2025-01-13 22:41:34 org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 125.0 failed 1 times, most recent failure: Lost task 0.0 in stage 125.0 (TID 2865) (ee7845301a1d executor driver): java.util.NoSuchElementException: None.get
2025-01-13 22:41:34     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:34     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:34     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:34     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:34     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:34     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:34     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:34     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:34     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:34     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:34     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:34     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:34     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:34     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:34     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:34     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:34     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:34     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:34     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:34     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:34     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:34     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:34 
2025-01-13 22:41:34 Driver stacktrace:
2025-01-13 22:41:34     at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
2025-01-13 22:41:34     at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
2025-01-13 22:41:34     at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
2025-01-13 22:41:34     at scala.collection.immutable.List.foreach(List.scala:333)
2025-01-13 22:41:34     at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
2025-01-13 22:41:34     at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
2025-01-13 22:41:34     at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
2025-01-13 22:41:34     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:34     at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
2025-01-13 22:41:34     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
2025-01-13 22:41:34     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
2025-01-13 22:41:34     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
2025-01-13 22:41:34     at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
2025-01-13 22:41:34     at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
2025-01-13 22:41:34     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
2025-01-13 22:41:34     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
2025-01-13 22:41:34     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
2025-01-13 22:41:34     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
2025-01-13 22:41:34     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)
2025-01-13 22:41:34     at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
2025-01-13 22:41:34     at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
2025-01-13 22:41:34     at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
2025-01-13 22:41:34     at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)
2025-01-13 22:41:34     at org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3514)
2025-01-13 22:41:34     at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
2025-01-13 22:41:34     at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4309)
2025-01-13 22:41:34     at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
2025-01-13 22:41:34     at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
2025-01-13 22:41:34     at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
2025-01-13 22:41:34     at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-01-13 22:41:34     at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
2025-01-13 22:41:34     at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4307)
2025-01-13 22:41:34     at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3514)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.write(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.SinkHandler$.writeMappingResult(SinkHandler.scala:37)
2025-01-13 22:41:34     at io.tofhir.engine.mapping.job.FhirMappingJobManager.$anonfun$readSourceExecuteAndWriteInChunks$1(FhirMappingJobManager.scala:326)
2025-01-13 22:41:34     at io.tofhir.engine.mapping.job.FhirMappingJobManager.$anonfun$readSourceExecuteAndWriteInChunks$1$adapted(FhirMappingJobManager.scala:326)
2025-01-13 22:41:34     at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
2025-01-13 22:41:34     at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:64)
2025-01-13 22:41:34     at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:101)
2025-01-13 22:41:34     at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
2025-01-13 22:41:34     at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:94)
2025-01-13 22:41:34     at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:101)
2025-01-13 22:41:34     at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2025-01-13 22:41:34     at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2025-01-13 22:41:34     at java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)
2025-01-13 22:41:34     at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source)
2025-01-13 22:41:34     at java.base/java.util.concurrent.ForkJoinPool.scan(Unknown Source)
2025-01-13 22:41:34     at java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
2025-01-13 22:41:34     at java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)
2025-01-13 22:41:34 Caused by: java.util.NoSuchElementException: None.get
2025-01-13 22:41:34     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:34     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:34     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:34     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:34     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:34     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:34     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:34     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:34     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:34     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:34     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:34     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:34     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:34     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:34     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:34     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:34     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:34     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:34     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:34     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:34     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:34     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:34     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:34 2025-01-13_19:41:34.526 [toFHIR-akka.actor.default-dispatcher-16] INFO  i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (STARTED) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'ips.measurement.laboratory'!
2025-01-13 22:41:34 
2025-01-13 22:41:34 2025-01-13_19:41:34.526 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Reading source data for mapping ips.measurement.laboratory within mapping job rarelink-cdm ...
2025-01-13 22:41:35 2025-01-13_19:41:35.389 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - 12 records read for mapping ips.measurement.laboratory within mapping job rarelink-cdm ...
2025-01-13 22:41:35 2025-01-13_19:41:35.390 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Executing the mapping ips.measurement.laboratory within job rarelink-cdm ...
2025-01-13 22:41:35 2025-01-13_19:41:35.451 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.d.w.FhirRepositoryWriter - Created FHIR resources will be written to the given FHIR repository URL:http://172.21.0.2:8080/fhir
2025-01-13 22:41:35 2025-01-13_19:41:35.618 [Executor task launch worker for task 0.0 in stage 131.0 (TID 2870)] DEBUG i.t.e.d.w.FhirRepositoryWriter - Batch Update request will be sent to the FHIR repository for 3 resources.
2025-01-13 22:41:35 2025-01-13_19:41:35.658 [Executor task launch worker for task 0.0 in stage 131.0 (TID 2870)] ERROR o.a.s.e.Executor - Exception in task 0.0 in stage 131.0 (TID 2870)
2025-01-13 22:41:35 java.util.NoSuchElementException: None.get
2025-01-13 22:41:35     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:35     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:35     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:35     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:35     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:35     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:35     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:35     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:35     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:35     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:35     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:35     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:35     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:35     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:35     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:35     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:35     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:35     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:35     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:35     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:35     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:35     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:35 2025-01-13_19:41:35.661 [task-result-getter-3] WARN  o.a.s.s.TaskSetManager - Lost task 0.0 in stage 131.0 (TID 2870) (ee7845301a1d executor driver): java.util.NoSuchElementException: None.get
2025-01-13 22:41:35     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:35     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:35     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:35     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:35     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:35     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:35     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:35     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:35     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:35     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:35     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:35     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:35     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:35     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:35     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:35     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:35     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:35     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:35     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:35     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:35     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:35     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:35 
2025-01-13 22:41:35 2025-01-13_19:41:35.662 [task-result-getter-3] ERROR o.a.s.s.TaskSetManager - Task 0 in stage 131.0 failed 1 times; aborting job
2025-01-13 22:41:35 2025-01-13_19:41:35.671 [toFHIR-akka.actor.default-dispatcher-16] ERROR i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (FAILURE) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'ips.measurement.laboratory'!
2025-01-13 22:41:35     # of Invalid Rows:      0
2025-01-13 22:41:35     # of Not Mapped:        0
2025-01-13 22:41:35     # of Failed writes:     0
2025-01-13 22:41:35     # of Written FHIR resources:    0
2025-01-13 22:41:35 org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 131.0 failed 1 times, most recent failure: Lost task 0.0 in stage 131.0 (TID 2870) (ee7845301a1d executor driver): java.util.NoSuchElementException: None.get
2025-01-13 22:41:35     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:35     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:35     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:35     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:35     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:35     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:35     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:35     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:35     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:35     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:35     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:35     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:35     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:35     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:35     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:35     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:35     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:35     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:35     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:35     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:35     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:35     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:35 
2025-01-13 22:41:35 Driver stacktrace:
2025-01-13 22:41:35     at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
2025-01-13 22:41:35     at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
2025-01-13 22:41:35     at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
2025-01-13 22:41:35     at scala.collection.immutable.List.foreach(List.scala:333)
2025-01-13 22:41:35     at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
2025-01-13 22:41:35     at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
2025-01-13 22:41:35     at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
2025-01-13 22:41:35     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:35     at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
2025-01-13 22:41:35     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
2025-01-13 22:41:35     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
2025-01-13 22:41:35     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
2025-01-13 22:41:35     at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
2025-01-13 22:41:35     at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
2025-01-13 22:41:35     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
2025-01-13 22:41:35     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
2025-01-13 22:41:35     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
2025-01-13 22:41:35     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
2025-01-13 22:41:35     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)
2025-01-13 22:41:35     at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
2025-01-13 22:41:35     at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
2025-01-13 22:41:35     at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
2025-01-13 22:41:35     at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)
2025-01-13 22:41:35     at org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3514)
2025-01-13 22:41:35     at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
2025-01-13 22:41:35     at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4309)
2025-01-13 22:41:35     at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
2025-01-13 22:41:35     at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
2025-01-13 22:41:35     at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
2025-01-13 22:41:35     at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-01-13 22:41:35     at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
2025-01-13 22:41:35     at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4307)
2025-01-13 22:41:35     at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3514)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.write(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.SinkHandler$.writeMappingResult(SinkHandler.scala:37)
2025-01-13 22:41:35     at io.tofhir.engine.mapping.job.FhirMappingJobManager.$anonfun$readSourceExecuteAndWriteInChunks$1(FhirMappingJobManager.scala:326)
2025-01-13 22:41:35     at io.tofhir.engine.mapping.job.FhirMappingJobManager.$anonfun$readSourceExecuteAndWriteInChunks$1$adapted(FhirMappingJobManager.scala:326)
2025-01-13 22:41:35     at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
2025-01-13 22:41:35     at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:64)
2025-01-13 22:41:35     at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:101)
2025-01-13 22:41:35     at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
2025-01-13 22:41:35     at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:94)
2025-01-13 22:41:35     at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:101)
2025-01-13 22:41:35     at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2025-01-13 22:41:35     at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2025-01-13 22:41:35     at java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)
2025-01-13 22:41:35     at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source)
2025-01-13 22:41:35     at java.base/java.util.concurrent.ForkJoinPool.scan(Unknown Source)
2025-01-13 22:41:35     at java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
2025-01-13 22:41:35     at java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)
2025-01-13 22:41:35 Caused by: java.util.NoSuchElementException: None.get
2025-01-13 22:41:35     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:35     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:35     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:35     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:35     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:35     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:35     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:35     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:35     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:35     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:35     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:35     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:35     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:35     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:35     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:35     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:35     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:35     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:35     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:35     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:35     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:35     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:35     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:35 2025-01-13_19:41:35.673 [toFHIR-akka.actor.default-dispatcher-16] INFO  i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (STARTED) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'ips.measurement.radiology'!
2025-01-13 22:41:35 
2025-01-13 22:41:35 2025-01-13_19:41:35.674 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Reading source data for mapping ips.measurement.radiology within mapping job rarelink-cdm ...
2025-01-13 22:41:36 2025-01-13_19:41:36.550 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - 12 records read for mapping ips.measurement.radiology within mapping job rarelink-cdm ...
2025-01-13 22:41:36 2025-01-13_19:41:36.556 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Executing the mapping ips.measurement.radiology within job rarelink-cdm ...
2025-01-13 22:41:36 2025-01-13_19:41:36.706 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.d.w.FhirRepositoryWriter - Created FHIR resources will be written to the given FHIR repository URL:http://172.21.0.2:8080/fhir
2025-01-13 22:41:36 2025-01-13_19:41:36.857 [Executor task launch worker for task 0.0 in stage 137.0 (TID 2875)] DEBUG i.t.e.d.w.FhirRepositoryWriter - Batch Update request will be sent to the FHIR repository for 3 resources.
2025-01-13 22:41:36 2025-01-13_19:41:36.888 [Executor task launch worker for task 0.0 in stage 137.0 (TID 2875)] ERROR o.a.s.e.Executor - Exception in task 0.0 in stage 137.0 (TID 2875)
2025-01-13 22:41:36 java.util.NoSuchElementException: None.get
2025-01-13 22:41:36     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:36     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:36     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:36     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:36     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:36     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:36     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:36     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:36     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:36     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:36     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:36     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:36     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:36     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:36     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:36     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:36     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:36     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:36     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:36     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:36     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:36     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:36 2025-01-13_19:41:36.890 [task-result-getter-2] WARN  o.a.s.s.TaskSetManager - Lost task 0.0 in stage 137.0 (TID 2875) (ee7845301a1d executor driver): java.util.NoSuchElementException: None.get
2025-01-13 22:41:36     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:36     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:36     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:36     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:36     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:36     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:36     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:36     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:36     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:36     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:36     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:36     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:36     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:36     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:36     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:36     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:36     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:36     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:36     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:36     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:36     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:36     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:36 
2025-01-13 22:41:36 2025-01-13_19:41:36.890 [task-result-getter-2] ERROR o.a.s.s.TaskSetManager - Task 0 in stage 137.0 failed 1 times; aborting job
2025-01-13 22:41:36 2025-01-13_19:41:36.897 [toFHIR-akka.actor.default-dispatcher-16] ERROR i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (FAILURE) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'ips.measurement.radiology'!
2025-01-13 22:41:36     # of Invalid Rows:      0
2025-01-13 22:41:36     # of Not Mapped:        0
2025-01-13 22:41:36     # of Failed writes:     0
2025-01-13 22:41:36     # of Written FHIR resources:    0
2025-01-13 22:41:36 org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 137.0 failed 1 times, most recent failure: Lost task 0.0 in stage 137.0 (TID 2875) (ee7845301a1d executor driver): java.util.NoSuchElementException: None.get
2025-01-13 22:41:36     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:36     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:36     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:36     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:36     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:36     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:36     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:36     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:36     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:36     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:36     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:36     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:36     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:36     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:36     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:36     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:36     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:36     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:36     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:36     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:36     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:36     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:36 
2025-01-13 22:41:36 Driver stacktrace:
2025-01-13 22:41:36     at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
2025-01-13 22:41:36     at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
2025-01-13 22:41:36     at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
2025-01-13 22:41:36     at scala.collection.immutable.List.foreach(List.scala:333)
2025-01-13 22:41:36     at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
2025-01-13 22:41:36     at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
2025-01-13 22:41:36     at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
2025-01-13 22:41:36     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:36     at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
2025-01-13 22:41:36     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
2025-01-13 22:41:36     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
2025-01-13 22:41:36     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
2025-01-13 22:41:36     at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
2025-01-13 22:41:36     at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
2025-01-13 22:41:36     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
2025-01-13 22:41:36     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
2025-01-13 22:41:36     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
2025-01-13 22:41:36     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
2025-01-13 22:41:36     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)
2025-01-13 22:41:36     at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
2025-01-13 22:41:36     at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
2025-01-13 22:41:36     at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
2025-01-13 22:41:36     at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)
2025-01-13 22:41:36     at org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3514)
2025-01-13 22:41:36     at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
2025-01-13 22:41:36     at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4309)
2025-01-13 22:41:36     at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
2025-01-13 22:41:36     at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
2025-01-13 22:41:36     at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
2025-01-13 22:41:36     at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-01-13 22:41:36     at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
2025-01-13 22:41:36     at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4307)
2025-01-13 22:41:36     at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3514)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.write(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.SinkHandler$.writeMappingResult(SinkHandler.scala:37)
2025-01-13 22:41:36     at io.tofhir.engine.mapping.job.FhirMappingJobManager.$anonfun$readSourceExecuteAndWriteInChunks$1(FhirMappingJobManager.scala:326)
2025-01-13 22:41:36     at io.tofhir.engine.mapping.job.FhirMappingJobManager.$anonfun$readSourceExecuteAndWriteInChunks$1$adapted(FhirMappingJobManager.scala:326)
2025-01-13 22:41:36     at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
2025-01-13 22:41:36     at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:64)
2025-01-13 22:41:36     at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:101)
2025-01-13 22:41:36     at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
2025-01-13 22:41:36     at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:94)
2025-01-13 22:41:36     at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:101)
2025-01-13 22:41:36     at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2025-01-13 22:41:36     at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2025-01-13 22:41:36     at java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)
2025-01-13 22:41:36     at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source)
2025-01-13 22:41:36     at java.base/java.util.concurrent.ForkJoinPool.scan(Unknown Source)
2025-01-13 22:41:36     at java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
2025-01-13 22:41:36     at java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)
2025-01-13 22:41:36 Caused by: java.util.NoSuchElementException: None.get
2025-01-13 22:41:36     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:36     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:36     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:36     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:36     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:36     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:36     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:36     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:36     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:36     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:36     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:36     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:36     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:36     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:36     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:36     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:36     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:36     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:36     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:36     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:36     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:36     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:36     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:36 2025-01-13_19:41:36.899 [toFHIR-akka.actor.default-dispatcher-16] INFO  i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (STARTED) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'observation.vitalsigns'!
2025-01-13 22:41:36 
2025-01-13 22:41:36 2025-01-13_19:41:36.899 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Reading source data for mapping observation.vitalsigns within mapping job rarelink-cdm ...
2025-01-13 22:41:37 2025-01-13_19:41:37.723 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - 12 records read for mapping observation.vitalsigns within mapping job rarelink-cdm ...
2025-01-13 22:41:37 2025-01-13_19:41:37.725 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Executing the mapping observation.vitalsigns within job rarelink-cdm ...
2025-01-13 22:41:37 2025-01-13_19:41:37.843 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.d.w.FhirRepositoryWriter - Created FHIR resources will be written to the given FHIR repository URL:http://172.21.0.2:8080/fhir
2025-01-13 22:41:37 2025-01-13_19:41:37.982 [Executor task launch worker for task 0.0 in stage 143.0 (TID 2880)] DEBUG i.t.e.d.w.FhirRepositoryWriter - Batch Update request will be sent to the FHIR repository for 4 resources.
2025-01-13 22:41:38 2025-01-13_19:41:38.028 [Executor task launch worker for task 0.0 in stage 143.0 (TID 2880)] ERROR o.a.s.e.Executor - Exception in task 0.0 in stage 143.0 (TID 2880)
2025-01-13 22:41:38 java.util.NoSuchElementException: None.get
2025-01-13 22:41:38     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:38     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:38     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:38     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:38     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:38     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:38     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:38     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:38     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:38     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:38     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:38     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:38     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:38     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:38     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:38     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:38     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:38     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:38     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:38     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:38     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:38     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:38 2025-01-13_19:41:38.030 [task-result-getter-0] WARN  o.a.s.s.TaskSetManager - Lost task 0.0 in stage 143.0 (TID 2880) (ee7845301a1d executor driver): java.util.NoSuchElementException: None.get
2025-01-13 22:41:38     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:38     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:38     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:38     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:38     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:38     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:38     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:38     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:38     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:38     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:38     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:38     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:38     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:38     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:38     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:38     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:38     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:38     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:38     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:38     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:38     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:38     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:38 
2025-01-13 22:41:38 2025-01-13_19:41:38.030 [task-result-getter-0] ERROR o.a.s.s.TaskSetManager - Task 0 in stage 143.0 failed 1 times; aborting job
2025-01-13 22:41:38 2025-01-13_19:41:38.036 [toFHIR-akka.actor.default-dispatcher-16] ERROR i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (FAILURE) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'observation.vitalsigns'!
2025-01-13 22:41:38     # of Invalid Rows:      0
2025-01-13 22:41:38     # of Not Mapped:        0
2025-01-13 22:41:38     # of Failed writes:     0
2025-01-13 22:41:38     # of Written FHIR resources:    0
2025-01-13 22:41:38 org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 143.0 failed 1 times, most recent failure: Lost task 0.0 in stage 143.0 (TID 2880) (ee7845301a1d executor driver): java.util.NoSuchElementException: None.get
2025-01-13 22:41:38     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:38     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:38     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:38     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:38     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:38     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:38     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:38     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:38     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:38     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:38     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:38     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:38     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:38     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:38     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:38     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:38     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:38     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:38     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:38     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:38     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:38     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:38 
2025-01-13 22:41:38 Driver stacktrace:
2025-01-13 22:41:38     at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
2025-01-13 22:41:38     at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
2025-01-13 22:41:38     at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
2025-01-13 22:41:38     at scala.collection.immutable.List.foreach(List.scala:333)
2025-01-13 22:41:38     at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
2025-01-13 22:41:38     at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
2025-01-13 22:41:38     at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
2025-01-13 22:41:38     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:38     at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
2025-01-13 22:41:38     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
2025-01-13 22:41:38     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
2025-01-13 22:41:38     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
2025-01-13 22:41:38     at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
2025-01-13 22:41:38     at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
2025-01-13 22:41:38     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
2025-01-13 22:41:38     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
2025-01-13 22:41:38     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
2025-01-13 22:41:38     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
2025-01-13 22:41:38     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)
2025-01-13 22:41:38     at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
2025-01-13 22:41:38     at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
2025-01-13 22:41:38     at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
2025-01-13 22:41:38     at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)
2025-01-13 22:41:38     at org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3514)
2025-01-13 22:41:38     at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
2025-01-13 22:41:38     at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4309)
2025-01-13 22:41:38     at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
2025-01-13 22:41:38     at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
2025-01-13 22:41:38     at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
2025-01-13 22:41:38     at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-01-13 22:41:38     at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
2025-01-13 22:41:38     at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4307)
2025-01-13 22:41:38     at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3514)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.write(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.SinkHandler$.writeMappingResult(SinkHandler.scala:37)
2025-01-13 22:41:38     at io.tofhir.engine.mapping.job.FhirMappingJobManager.$anonfun$readSourceExecuteAndWriteInChunks$1(FhirMappingJobManager.scala:326)
2025-01-13 22:41:38     at io.tofhir.engine.mapping.job.FhirMappingJobManager.$anonfun$readSourceExecuteAndWriteInChunks$1$adapted(FhirMappingJobManager.scala:326)
2025-01-13 22:41:38     at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
2025-01-13 22:41:38     at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:64)
2025-01-13 22:41:38     at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:101)
2025-01-13 22:41:38     at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
2025-01-13 22:41:38     at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:94)
2025-01-13 22:41:38     at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:101)
2025-01-13 22:41:38     at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2025-01-13 22:41:38     at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2025-01-13 22:41:38     at java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)
2025-01-13 22:41:38     at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source)
2025-01-13 22:41:38     at java.base/java.util.concurrent.ForkJoinPool.scan(Unknown Source)
2025-01-13 22:41:38     at java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
2025-01-13 22:41:38     at java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)
2025-01-13 22:41:38 Caused by: java.util.NoSuchElementException: None.get
2025-01-13 22:41:38     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:38     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:38     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:38     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:38     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:38     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:38     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:38     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:38     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:38     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:38     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:38     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:38     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:38     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:38     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:38     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:38     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:38     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:38     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:38     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:38     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:38     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:38     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:38 2025-01-13_19:41:38.036 [toFHIR-akka.actor.default-dispatcher-16] INFO  i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (STARTED) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'ips.procedure'!
2025-01-13 22:41:38 
2025-01-13 22:41:38 2025-01-13_19:41:38.036 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Reading source data for mapping ips.procedure within mapping job rarelink-cdm ...
2025-01-13 22:41:38 2025-01-13_19:41:38.825 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - 12 records read for mapping ips.procedure within mapping job rarelink-cdm ...
2025-01-13 22:41:38 2025-01-13_19:41:38.825 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Executing the mapping ips.procedure within job rarelink-cdm ...
2025-01-13 22:41:38 2025-01-13_19:41:38.873 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.d.w.FhirRepositoryWriter - Created FHIR resources will be written to the given FHIR repository URL:http://172.21.0.2:8080/fhir
2025-01-13 22:41:39 2025-01-13_19:41:39.015 [Executor task launch worker for task 0.0 in stage 149.0 (TID 2885)] DEBUG i.t.e.d.w.FhirRepositoryWriter - Batch Update request will be sent to the FHIR repository for 1 resources.
2025-01-13 22:41:39 2025-01-13_19:41:39.043 [Executor task launch worker for task 0.0 in stage 149.0 (TID 2885)] ERROR o.a.s.e.Executor - Exception in task 0.0 in stage 149.0 (TID 2885)
2025-01-13 22:41:39 java.util.NoSuchElementException: None.get
2025-01-13 22:41:39     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:39     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:39     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:39     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:39     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:39     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:39     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:39     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:39     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:39     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:39     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:39     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:39     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:39     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:39     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:39     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:39     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:39     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:39     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:39     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:39     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:39     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:39 2025-01-13_19:41:39.044 [task-result-getter-1] WARN  o.a.s.s.TaskSetManager - Lost task 0.0 in stage 149.0 (TID 2885) (ee7845301a1d executor driver): java.util.NoSuchElementException: None.get
2025-01-13 22:41:39     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:39     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:39     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:39     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:39     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:39     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:39     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:39     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:39     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:39     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:39     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:39     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:39     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:39     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:39     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:39     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:39     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:39     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:39     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:39     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:39     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:39     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:39 
2025-01-13 22:41:39 2025-01-13_19:41:39.044 [task-result-getter-1] ERROR o.a.s.s.TaskSetManager - Task 0 in stage 149.0 failed 1 times; aborting job
2025-01-13 22:41:39 2025-01-13_19:41:39.050 [toFHIR-akka.actor.default-dispatcher-16] ERROR i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (FAILURE) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'ips.procedure'!
2025-01-13 22:41:39     # of Invalid Rows:      0
2025-01-13 22:41:39     # of Not Mapped:        0
2025-01-13 22:41:39     # of Failed writes:     0
2025-01-13 22:41:39     # of Written FHIR resources:    0
2025-01-13 22:41:39 org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 149.0 failed 1 times, most recent failure: Lost task 0.0 in stage 149.0 (TID 2885) (ee7845301a1d executor driver): java.util.NoSuchElementException: None.get
2025-01-13 22:41:39     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:39     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:39     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:39     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:39     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:39     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:39     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:39     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:39     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:39     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:39     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:39     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:39     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:39     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:39     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:39     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:39     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:39     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:39     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:39     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:39     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:39     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:39 
2025-01-13 22:41:39 Driver stacktrace:
2025-01-13 22:41:39     at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
2025-01-13 22:41:39     at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
2025-01-13 22:41:39     at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
2025-01-13 22:41:39     at scala.collection.immutable.List.foreach(List.scala:333)
2025-01-13 22:41:39     at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
2025-01-13 22:41:39     at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
2025-01-13 22:41:39     at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
2025-01-13 22:41:39     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:39     at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
2025-01-13 22:41:39     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
2025-01-13 22:41:39     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
2025-01-13 22:41:39     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
2025-01-13 22:41:39     at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
2025-01-13 22:41:39     at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
2025-01-13 22:41:39     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
2025-01-13 22:41:39     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
2025-01-13 22:41:39     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
2025-01-13 22:41:39     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
2025-01-13 22:41:39     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)
2025-01-13 22:41:39     at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
2025-01-13 22:41:39     at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
2025-01-13 22:41:39     at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
2025-01-13 22:41:39     at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)
2025-01-13 22:41:39     at org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3514)
2025-01-13 22:41:39     at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
2025-01-13 22:41:39     at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4309)
2025-01-13 22:41:39     at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
2025-01-13 22:41:39     at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
2025-01-13 22:41:39     at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
2025-01-13 22:41:39     at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-01-13 22:41:39     at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
2025-01-13 22:41:39     at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4307)
2025-01-13 22:41:39     at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3514)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.write(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.SinkHandler$.writeMappingResult(SinkHandler.scala:37)
2025-01-13 22:41:39     at io.tofhir.engine.mapping.job.FhirMappingJobManager.$anonfun$readSourceExecuteAndWriteInChunks$1(FhirMappingJobManager.scala:326)
2025-01-13 22:41:39     at io.tofhir.engine.mapping.job.FhirMappingJobManager.$anonfun$readSourceExecuteAndWriteInChunks$1$adapted(FhirMappingJobManager.scala:326)
2025-01-13 22:41:39     at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
2025-01-13 22:41:39     at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:64)
2025-01-13 22:41:39     at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:101)
2025-01-13 22:41:39     at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
2025-01-13 22:41:39     at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:94)
2025-01-13 22:41:39     at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:101)
2025-01-13 22:41:39     at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2025-01-13 22:41:39     at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2025-01-13 22:41:39     at java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)
2025-01-13 22:41:39     at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source)
2025-01-13 22:41:39     at java.base/java.util.concurrent.ForkJoinPool.scan(Unknown Source)
2025-01-13 22:41:39     at java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
2025-01-13 22:41:39     at java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)
2025-01-13 22:41:39 Caused by: java.util.NoSuchElementException: None.get
2025-01-13 22:41:39     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:39     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:39     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:39     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:39     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:39     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:39     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:39     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:39     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:39     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:39     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:39     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:39     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:39     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:39     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:39     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:39     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:39     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:39     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:39     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:39     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:39     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:39     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:39 2025-01-13_19:41:39.051 [toFHIR-akka.actor.default-dispatcher-16] INFO  i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (STARTED) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'observation.measurements.other'!
2025-01-13 22:41:39 
2025-01-13 22:41:39 2025-01-13_19:41:39.052 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Reading source data for mapping observation.measurements.other within mapping job rarelink-cdm ...
2025-01-13 22:41:39 2025-01-13_19:41:39.802 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - 12 records read for mapping observation.measurements.other within mapping job rarelink-cdm ...
2025-01-13 22:41:39 2025-01-13_19:41:39.802 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Executing the mapping observation.measurements.other within job rarelink-cdm ...
2025-01-13 22:41:39 2025-01-13_19:41:39.869 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.d.w.FhirRepositoryWriter - Created FHIR resources will be written to the given FHIR repository URL:http://172.21.0.2:8080/fhir
2025-01-13 22:41:39 line 1:8 token recognition error at: '''
2025-01-13 22:41:40 2025-01-13_19:41:40.017 [Executor task launch worker for task 0.0 in stage 155.0 (TID 2890)] DEBUG i.t.e.d.w.FhirRepositoryWriter - Batch Update request will be sent to the FHIR repository for 1 resources.
2025-01-13 22:41:40 2025-01-13_19:41:40.036 [Executor task launch worker for task 0.0 in stage 155.0 (TID 2890)] ERROR o.a.s.e.Executor - Exception in task 0.0 in stage 155.0 (TID 2890)
2025-01-13 22:41:40 java.util.NoSuchElementException: None.get
2025-01-13 22:41:40     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:40     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:40     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:40     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:40     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:40     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:40     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:40     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:40     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:40     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:40     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:40     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:40     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:40     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:40     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:40     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:40     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:40     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:40     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:40     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:40     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:40     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:40 2025-01-13_19:41:40.043 [task-result-getter-3] WARN  o.a.s.s.TaskSetManager - Lost task 0.0 in stage 155.0 (TID 2890) (ee7845301a1d executor driver): java.util.NoSuchElementException: None.get
2025-01-13 22:41:40     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:40     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:40     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:40     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:40     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:40     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:40     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:40     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:40     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:40     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:40     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:40     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:40     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:40     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:40     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:40     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:40     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:40     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:40     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:40     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:40     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:40     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:40 
2025-01-13 22:41:40 2025-01-13_19:41:40.044 [task-result-getter-3] ERROR o.a.s.s.TaskSetManager - Task 0 in stage 155.0 failed 1 times; aborting job
2025-01-13 22:41:40 2025-01-13_19:41:40.050 [toFHIR-akka.actor.default-dispatcher-16] ERROR i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (FAILURE) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'observation.measurements.other'!
2025-01-13 22:41:40     # of Invalid Rows:      0
2025-01-13 22:41:40     # of Not Mapped:        0
2025-01-13 22:41:40     # of Failed writes:     0
2025-01-13 22:41:40     # of Written FHIR resources:    0
2025-01-13 22:41:40 org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 155.0 failed 1 times, most recent failure: Lost task 0.0 in stage 155.0 (TID 2890) (ee7845301a1d executor driver): java.util.NoSuchElementException: None.get
2025-01-13 22:41:40     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:40     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:40     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:40     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:40     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:40     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:40     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:40     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:40     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:40     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:40     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:40     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:40     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:40     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:40     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:40     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:40     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:40     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:40     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:40     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:40     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:40     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:40 
2025-01-13 22:41:40 Driver stacktrace:
2025-01-13 22:41:40     at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
2025-01-13 22:41:40     at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
2025-01-13 22:41:40     at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
2025-01-13 22:41:40     at scala.collection.immutable.List.foreach(List.scala:333)
2025-01-13 22:41:40     at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
2025-01-13 22:41:40     at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
2025-01-13 22:41:40     at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
2025-01-13 22:41:40     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:40     at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
2025-01-13 22:41:40     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
2025-01-13 22:41:40     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
2025-01-13 22:41:40     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
2025-01-13 22:41:40     at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
2025-01-13 22:41:40     at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
2025-01-13 22:41:40     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
2025-01-13 22:41:40     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
2025-01-13 22:41:40     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
2025-01-13 22:41:40     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
2025-01-13 22:41:40     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)
2025-01-13 22:41:40     at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
2025-01-13 22:41:40     at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
2025-01-13 22:41:40     at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
2025-01-13 22:41:40     at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)
2025-01-13 22:41:40     at org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3514)
2025-01-13 22:41:40     at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
2025-01-13 22:41:40     at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4309)
2025-01-13 22:41:40     at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
2025-01-13 22:41:40     at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
2025-01-13 22:41:40     at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
2025-01-13 22:41:40     at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-01-13 22:41:40     at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
2025-01-13 22:41:40     at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4307)
2025-01-13 22:41:40     at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3514)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.write(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.SinkHandler$.writeMappingResult(SinkHandler.scala:37)
2025-01-13 22:41:40     at io.tofhir.engine.mapping.job.FhirMappingJobManager.$anonfun$readSourceExecuteAndWriteInChunks$1(FhirMappingJobManager.scala:326)
2025-01-13 22:41:40     at io.tofhir.engine.mapping.job.FhirMappingJobManager.$anonfun$readSourceExecuteAndWriteInChunks$1$adapted(FhirMappingJobManager.scala:326)
2025-01-13 22:41:40     at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
2025-01-13 22:41:40     at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:64)
2025-01-13 22:41:40     at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:101)
2025-01-13 22:41:40     at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
2025-01-13 22:41:40     at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:94)
2025-01-13 22:41:40     at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:101)
2025-01-13 22:41:40     at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2025-01-13 22:41:40     at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2025-01-13 22:41:40     at java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)
2025-01-13 22:41:40     at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source)
2025-01-13 22:41:40     at java.base/java.util.concurrent.ForkJoinPool.scan(Unknown Source)
2025-01-13 22:41:40     at java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
2025-01-13 22:41:40     at java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)
2025-01-13 22:41:40 Caused by: java.util.NoSuchElementException: None.get
2025-01-13 22:41:40     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:40     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:40     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:40     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:40     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:40     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:40     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:40     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:40     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:40     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:40     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:40     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:40     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:40     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:40     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:40     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:40     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:40     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:40     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:40     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:40     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:40     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:40     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:40 2025-01-13_19:41:40.050 [toFHIR-akka.actor.default-dispatcher-16] INFO  i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (STARTED) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'familymemberhistory.familyhistory'!
2025-01-13 22:41:40 
2025-01-13 22:41:40 2025-01-13_19:41:40.051 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Reading source data for mapping familymemberhistory.familyhistory within mapping job rarelink-cdm ...
2025-01-13 22:41:40 2025-01-13_19:41:40.886 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - 5 records read for mapping familymemberhistory.familyhistory within mapping job rarelink-cdm ...
2025-01-13 22:41:40 2025-01-13_19:41:40.887 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Executing the mapping familymemberhistory.familyhistory within job rarelink-cdm ...
2025-01-13 22:41:40 2025-01-13_19:41:40.957 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.d.w.FhirRepositoryWriter - Created FHIR resources will be written to the given FHIR repository URL:http://172.21.0.2:8080/fhir
2025-01-13 22:41:41 2025-01-13_19:41:41.210 [Executor task launch worker for task 0.0 in stage 161.0 (TID 2895)] DEBUG i.t.e.d.w.FhirRepositoryWriter - Batch Update request will be sent to the FHIR repository for 5 resources.
2025-01-13 22:41:41 2025-01-13_19:41:41.256 [Executor task launch worker for task 0.0 in stage 161.0 (TID 2895)] ERROR o.a.s.e.Executor - Exception in task 0.0 in stage 161.0 (TID 2895)
2025-01-13 22:41:41 java.util.NoSuchElementException: None.get
2025-01-13 22:41:41     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:41     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:41     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:41     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:41     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:41     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:41     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:41     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:41     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:41     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:41     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:41     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:41     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:41     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:41     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:41     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:41     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:41     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:41     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:41     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:41     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:41     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:41 2025-01-13_19:41:41.258 [task-result-getter-2] WARN  o.a.s.s.TaskSetManager - Lost task 0.0 in stage 161.0 (TID 2895) (ee7845301a1d executor driver): java.util.NoSuchElementException: None.get
2025-01-13 22:41:41     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:41     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:41     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:41     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:41     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:41     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:41     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:41     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:41     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:41     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:41     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:41     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:41     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:41     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:41     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:41 line 1:0 token recognition error at: '?'
2025-01-13 22:41:41 line 1:0 token recognition error at: '?'
2025-01-13 22:41:41 line 1:0 token recognition error at: '?'
2025-01-13 22:41:41 line 1:0 token recognition error at: '?'
2025-01-13 22:41:41 line 1:0 token recognition error at: '?'
2025-01-13 22:41:41 line 1:0 token recognition error at: '?'
2025-01-13 22:41:41 line 1:0 token recognition error at: '?'
2025-01-13 22:41:41 line 1:0 token recognition error at: '?'
2025-01-13 22:41:41 line 1:0 token recognition error at: '?'
2025-01-13 22:41:41 line 1:0 token recognition error at: '?'
2025-01-13 22:41:41 line 1:0 token recognition error at: '?'
2025-01-13 22:41:41 line 1:0 token recognition error at: '?'
2025-01-13 22:41:41 line 1:0 token recognition error at: '?'
2025-01-13 22:41:41 line 1:0 token recognition error at: '?'
2025-01-13 22:41:41 line 1:0 token recognition error at: '?'
2025-01-13 22:41:41 line 1:0 token recognition error at: '?'
2025-01-13 22:41:41 line 1:0 token recognition error at: '?'
2025-01-13 22:41:41 line 1:0 token recognition error at: '?'
2025-01-13 22:41:41 line 1:0 token recognition error at: '?'
2025-01-13 22:41:41 line 1:0 token recognition error at: '?'
2025-01-13 22:41:41     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:41     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:41     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:41     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:41     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:41     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:41     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:41 
2025-01-13 22:41:41 2025-01-13_19:41:41.259 [task-result-getter-2] ERROR o.a.s.s.TaskSetManager - Task 0 in stage 161.0 failed 1 times; aborting job
2025-01-13 22:41:41 2025-01-13_19:41:41.264 [toFHIR-akka.actor.default-dispatcher-16] ERROR i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (FAILURE) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'familymemberhistory.familyhistory'!
2025-01-13 22:41:41     # of Invalid Rows:      0
2025-01-13 22:41:41     # of Not Mapped:        0
2025-01-13 22:41:41     # of Failed writes:     0
2025-01-13 22:41:41     # of Written FHIR resources:    0
2025-01-13 22:41:41 org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 161.0 failed 1 times, most recent failure: Lost task 0.0 in stage 161.0 (TID 2895) (ee7845301a1d executor driver): java.util.NoSuchElementException: None.get
2025-01-13 22:41:41     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:41     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:41     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:41     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:41     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:41     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:41     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:41     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:41     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:41     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:41     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:41     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:41     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:41     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:41     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:41     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:41     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:41     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:41     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:41     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:41     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:41     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:41 
2025-01-13 22:41:41 Driver stacktrace:
2025-01-13 22:41:41     at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
2025-01-13 22:41:41     at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
2025-01-13 22:41:41     at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
2025-01-13 22:41:41     at scala.collection.immutable.List.foreach(List.scala:333)
2025-01-13 22:41:41     at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
2025-01-13 22:41:41     at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
2025-01-13 22:41:41     at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
2025-01-13 22:41:41     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:41     at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
2025-01-13 22:41:41     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
2025-01-13 22:41:41     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
2025-01-13 22:41:41     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
2025-01-13 22:41:41     at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
2025-01-13 22:41:41     at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
2025-01-13 22:41:41     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
2025-01-13 22:41:41     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
2025-01-13 22:41:41     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
2025-01-13 22:41:41     at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
2025-01-13 22:41:41     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1039)
2025-01-13 22:41:41     at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
2025-01-13 22:41:41     at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
2025-01-13 22:41:41     at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
2025-01-13 22:41:41     at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1037)
2025-01-13 22:41:41     at org.apache.spark.sql.Dataset.$anonfun$foreachPartition$1(Dataset.scala:3514)
2025-01-13 22:41:41     at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
2025-01-13 22:41:41     at org.apache.spark.sql.Dataset.$anonfun$withNewRDDExecutionId$1(Dataset.scala:4309)
2025-01-13 22:41:41     at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
2025-01-13 22:41:41     at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
2025-01-13 22:41:41     at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
2025-01-13 22:41:41     at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
2025-01-13 22:41:41     at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
2025-01-13 22:41:41     at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:4307)
2025-01-13 22:41:41     at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:3514)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.write(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.SinkHandler$.writeMappingResult(SinkHandler.scala:37)
2025-01-13 22:41:41     at io.tofhir.engine.mapping.job.FhirMappingJobManager.$anonfun$readSourceExecuteAndWriteInChunks$1(FhirMappingJobManager.scala:326)
2025-01-13 22:41:41     at io.tofhir.engine.mapping.job.FhirMappingJobManager.$anonfun$readSourceExecuteAndWriteInChunks$1$adapted(FhirMappingJobManager.scala:326)
2025-01-13 22:41:41     at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
2025-01-13 22:41:41     at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:64)
2025-01-13 22:41:41     at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:101)
2025-01-13 22:41:41     at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
2025-01-13 22:41:41     at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:94)
2025-01-13 22:41:41     at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:101)
2025-01-13 22:41:41     at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2025-01-13 22:41:41     at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2025-01-13 22:41:41     at java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)
2025-01-13 22:41:41     at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(Unknown Source)
2025-01-13 22:41:41     at java.base/java.util.concurrent.ForkJoinPool.scan(Unknown Source)
2025-01-13 22:41:41     at java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
2025-01-13 22:41:41     at java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)
2025-01-13 22:41:41 Caused by: java.util.NoSuchElementException: None.get
2025-01-13 22:41:41     at scala.None$.get(Option.scala:627)
2025-01-13 22:41:41     at scala.None$.get(Option.scala:626)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$getNonTransientErrorUUIDs$3(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:41     at scala.collection.immutable.List.map(List.scala:246)
2025-01-13 22:41:41     at scala.collection.immutable.List.map(List.scala:79)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.getNonTransientErrorUUIDs(FhirRepositoryWriter.scala:326)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.checkResults(FhirRepositoryWriter.scala:280)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4(FhirRepositoryWriter.scala:58)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$4$adapted(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:41     at scala.Option.foreach(Option.scala:437)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2(FhirRepositoryWriter.scala:57)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$2$adapted(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:41     at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)
2025-01-13 22:41:41     at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)
2025-01-13 22:41:41     at scala.collection.AbstractIterator.foreach(Iterator.scala:1293)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1(FhirRepositoryWriter.scala:48)
2025-01-13 22:41:41     at io.tofhir.engine.data.write.FhirRepositoryWriter.$anonfun$write$1$adapted(FhirRepositoryWriter.scala:42)
2025-01-13 22:41:41     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1039)
2025-01-13 22:41:41     at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1039)
2025-01-13 22:41:41     at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
2025-01-13 22:41:41     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
2025-01-13 22:41:41     at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
2025-01-13 22:41:41     at org.apache.spark.scheduler.Task.run(Task.scala:141)
2025-01-13 22:41:41     at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
2025-01-13 22:41:41     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
2025-01-13 22:41:41     at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
2025-01-13 22:41:41     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
2025-01-13 22:41:41     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
2025-01-13 22:41:41     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2025-01-13 22:41:41     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2025-01-13 22:41:41     at java.base/java.lang.Thread.run(Unknown Source)
2025-01-13 22:41:41 2025-01-13_19:41:41.267 [toFHIR-akka.actor.default-dispatcher-16] INFO  i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (STARTED) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'consent.consent'!
2025-01-13 22:41:41 
2025-01-13 22:41:41 2025-01-13_19:41:41.269 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Reading source data for mapping consent.consent within mapping job rarelink-cdm ...
2025-01-13 22:41:42 2025-01-13_19:41:42.070 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - 5 records read for mapping consent.consent within mapping job rarelink-cdm ...
2025-01-13 22:41:42 2025-01-13_19:41:42.070 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - Executing the mapping consent.consent within job rarelink-cdm ...
2025-01-13 22:41:42 2025-01-13_19:41:42.136 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.d.w.FhirRepositoryWriter - Created FHIR resources will be written to the given FHIR repository URL:http://172.21.0.2:8080/fhir
2025-01-13 22:41:42 2025-01-13_19:41:42.261 [Executor task launch worker for task 0.0 in stage 167.0 (TID 2900)] DEBUG i.t.e.d.w.FhirRepositoryWriter - Batch Update request will be sent to the FHIR repository for 5 resources.
2025-01-13 22:41:42 2025-01-13_19:41:42.333 [Executor task launch worker for task 0.0 in stage 167.0 (TID 2900)] DEBUG i.t.e.d.w.FhirRepositoryWriter - 5 FHIR resources were written to the FHIR repository successfully.
2025-01-13 22:41:42 2025-01-13_19:41:42.571 [toFHIR-akka.actor.default-dispatcher-16] INFO  i.t.e.e.l.ExecutionLogger$ - toFHIR chunk mapping result (SUCCESS) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'consent.consent'!
2025-01-13 22:41:42     # of Invalid Rows:      0
2025-01-13 22:41:42     # of Not Mapped:        0
2025-01-13 22:41:42     # of Failed writes:     0
2025-01-13 22:41:42     # of Written FHIR resources:    5
2025-01-13 22:41:42 2025-01-13_19:41:42.576 [toFHIR-akka.actor.default-dispatcher-16] INFO  i.t.e.e.l.ExecutionLogger$ - toFHIR mapping result (SUCCESS) for execution 'd6b3bb8b-a5c9-4ffd-8cd3-b6a7adec27d3' of job 'rarelink-cdm' in project '' for mappingTask 'consent.consent'!
2025-01-13 22:41:42     # of Invalid Rows:      0
2025-01-13 22:41:42     # of Not Mapped:        0
2025-01-13 22:41:42     # of Failed writes:     0
2025-01-13 22:41:42     # of Written FHIR resources:    5
2025-01-13 22:41:42 2025-01-13_19:41:42.577 [toFHIR-akka.actor.default-dispatcher-16] DEBUG i.t.e.m.j.FhirMappingJobManager - MappingJob execution finished for MappingJob: rarelink-cdm.
